<!DOCTYPE html><html><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width"/><meta name="next-head-count" content="2"/><link rel="preload" href="./_next/static/css/d1893b45984f5972.css" as="style"/><link rel="stylesheet" href="./_next/static/css/d1893b45984f5972.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="./_next/static/chunks/polyfills-42372ed130431b0a.js"></script><script src="./_next/static/chunks/webpack-6f0e0dcf15b6875d.js" defer=""></script><script src="./_next/static/chunks/framework-49c6cecf1f6d5795.js" defer=""></script><script src="./_next/static/chunks/main-4480e7d19afaa09f.js" defer=""></script><script src="./_next/static/chunks/pages/_app-4ac3cada10d020c2.js" defer=""></script><script src="./_next/static/chunks/413-9ff854af82ce4d76.js" defer=""></script><script src="./_next/static/chunks/664-d254d21a6fe56bff.js" defer=""></script><script src="./_next/static/chunks/39-45fee9698b3cd58c.js" defer=""></script><script src="./_next/static/chunks/pages/index-4106543ad8e86225.js" defer=""></script><script src="./_next/static/pksff7eA2sBIC06Rubinb/_buildManifest.js" defer=""></script><script src="./_next/static/pksff7eA2sBIC06Rubinb/_ssgManifest.js" defer=""></script></head><body><div id="__next"></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"result":{"clusters":[{"cluster":"クリエイティブなものづくりのプラットフォーム","cluster_id":"1","takeaways":"FabCafe Tokyoは2012年にオープンし、日本初のデジタルものづくりカフェとして注目を集めました。多様なプロフェッショナルが集まるこの場所は、クリエイター同士のコラボレーションを促進し、新たなプロジェクトが生まれる実験の場となっています。オープン当初は集客に苦労しましたが、現在ではノマドワーカーや学生、イベント参加者が集まる多機能なスペースとして成長しました。\n\nFabCafeは国内外に拠点を広げ、各地で地域の文化や産業資源を活かしたユニークなプロジェクトを展開しています。飲食、デジタル工作機械サービス、イベント、ワークショップなど多様なサービスを提供し、特にイベント系の比重が高いのが特徴です。また、企業の新規事業開発やR\u0026Dにも活用され、異分野の専門家が集まるハブとして機能しています。最近では、宇宙探査プロジェクト「Project Apophis」など、革新的な取り組みも進行中です。","arguments":[{"arg_id":"A0_0","argument":"FabCafeは、デジタル製造（FAB）のムーブメントを日常の場で体験できるようにするため、2012年に東京・渋谷で誕生した世界初の「レーザーカッターが主役」のものづくりカフェです。","comment_id":"0","x":5.195172,"y":9.716512,"p":0.8128455380349444},{"arg_id":"A2_0","argument":"FabCafeは、このグローバルに広がる“ものづくり革命”の精神（FABスピリット）を日本でも楽しく美味しく分かりやすく伝える場として企画されました。","comment_id":"2","x":5.043706,"y":8.546135,"p":0.8219844717055945},{"arg_id":"A3_0","argument":"若者のエネルギーがあふれる渋谷という立地を選び、レーザーカッターをはじめ様々なデジタル工作機器を備えることで、多くの人がワクワクしながら新しいものづくりを楽しめる空間を目指しました。","comment_id":"3","x":5.310509,"y":10.350416,"p":0.86708299851297},{"arg_id":"A4_0","argument":"FabCafe Tokyo（渋谷道玄坂）は2012年3月7日にオープンし、日本初のデジタルものづくりカフェとして話題を集めました。","comment_id":"4","x":4.1903,"y":9.23996,"p":0},{"arg_id":"A4_1","argument":"当初店舗面積40席のカフェスペースにレーザーカッターを据え、来店客が手軽にデジタル工作を体験できるようにしました。","comment_id":"4","x":5.4825172,"y":10.291628,"p":1},{"arg_id":"A5_0","argument":"FabCafeのコンセプトは「What do you Fab?（あなたは何をFabしますか？）」で、iPhoneケースやグリーティングカード、アクセサリー、椅子、照明、果ては家のモデルまで、様々なものをデジタル加工で“Fab”できることを示しました。","comment_id":"5","x":5.1988645,"y":8.581275,"p":1},{"arg_id":"A9_0","argument":"FabCafeは通常のカフェとは一線を画し、建築家・イラストレーター・エンジニア・アーティスト・研究者・ハッカー・アクティビスト・ドローンパイロットなど多彩なプロフェッショナルが集う「秘密基地」のような存在となりました。","comment_id":"9","x":5.9010663,"y":9.759671,"p":1},{"arg_id":"A10_0","argument":"異なる分野のクリエイター同士が交わることで日々化学反応が起こり、新たなプロジェクトが次々と生まれる実験の場――それがFabCafeの目指した世界観です。","comment_id":"10","x":5.2847047,"y":9.072721,"p":0.955862203712701},{"arg_id":"A11_0","argument":"FabCafeは誕生当初、コーヒーを淹れられるバリスタ不在時にはコーヒー提供を休止したり、工作機械を扱えるスタッフ不在時には「マシンメンテナンス中」の札を出すなど、手探りで運営を開始しました。","comment_id":"11","x":6.1596045,"y":10.812573,"p":0.7957019988725546},{"arg_id":"A12_0","argument":"オープン直後は立地条件（渋谷・道玄坂上という一等地ではあるが人通りは限られる）もあって集客に苦労し、店内が閑散とする日も多く、スタッフ総出で街頭でチラシ配りをするところからスタートしました。","comment_id":"12","x":5.00835,"y":10.79908,"p":0},{"arg_id":"A13_0","argument":"開店当初は赤字続きで、「店を続けること」自体が大きなチャレンジの連続でしたが、徐々にサービス改善やオペレーション改善を重ねていきました。","comment_id":"13","x":5.247182,"y":11.046112,"p":0},{"arg_id":"A15_0","argument":"平日の昼間はノマドワーカーや学生が電源・WiFi完備のカフェ空間を利用し、夜や週末にはものづくり系のイベントやワークショップが頻繁に開催されるという具合に、時間帯によって様々な顔を持つ場所となっていきました。","comment_id":"15","x":5.386658,"y":10.631108,"p":0},{"arg_id":"A17_0","argument":"FabCafeの印象は人によって「美味しいコーヒーやおしゃれなスイーツが楽しめるカフェ」から「3Dプリンタがあるメイカースペース」「いつも外国人が集まっていてイベントをやっている場所」「グローバルなクリエイターコミュニティのハブ」まで様々ですが、どれもFabCafeの一面を表しています。","comment_id":"17","x":5.845592,"y":9.848413,"p":1},{"arg_id":"A19_0","argument":"カフェとしてお客様の回転率を上げる効率よりも、クリエイター（＝デザインしたりモノづくりできる人）を惹きつけるために最新のデジタル工作マシンを導入し、魅力的なイベントやワークショップを頻繁に開催する方針をとっています。","comment_id":"19","x":5.7219276,"y":10.37086,"p":1},{"arg_id":"A22_0","argument":"私たちは、クリエイターが集まり新しいものづくりのコラボレーションが起きる“装置”としてのFabCafeから、次世代のものづくりを変革するイノベーションが生まれるはずだと本気で信じています。","comment_id":"22","x":5.489236,"y":8.904826,"p":0.8419483271944593},{"arg_id":"A25_0","argument":"FabCafeは「実験の場」を自称しており、定型化されたプロジェクトだけでなく、実験段階のテーマやアイデアにも果敢に取り組みます。","comment_id":"25","x":6.1471148,"y":9.34427,"p":0.9072879620237504},{"arg_id":"A26_0","argument":"少人数でニッチなワークショップの設計からスタートし、そこからムーブメントの起爆剤となるような仕掛けを生み出すこともFabCafeの使命の一部です。","comment_id":"26","x":5.3738136,"y":9.182186,"p":1},{"arg_id":"A34_0","argument":"FabCafeの公式サイトには「Open a FabCafe in your city and join us!（あなたの街にFabCafeを作ろう、仲間に加わろう！）」という驚くほど気軽な問い合わせフォームが設置されており、月に数件は世界中から「自分の街でFabCafeを開きたい」というメールが届くほどです。","comment_id":"34","x":6.3058863,"y":11.038834,"p":1},{"arg_id":"A39_0","argument":"FabCafeが掲げる「マニアックな『面白い』を世の中にひらいていく」というビジョンは、言い換えれば専門領域の知見をオープンに共有し社会課題解決につなげることです。このビジョンのもと、各FabCafeでは先端テクノロジーや文化と地域コミュニティを交差させるユニークな取り組みが実践されています。","comment_id":"39","x":6.300305,"y":9.088451,"p":1},{"arg_id":"A40_0","argument":"2012年の東京（渋谷）開業以降、FabCafeは日本国内外で拠点を増やし続けています。2020年時点で世界8カ国・11拠点に広がり、2022年10月現在では世界14拠点に達するグローバルコミュニティとなりました。","comment_id":"40","x":4.168657,"y":8.594269,"p":1},{"arg_id":"A43_0","argument":"FabCafeの各拠点は、提供するサービスのバランスが異なります。FabCafeの事業は大きく「飲食（ドリンク＆フード）」「デジタル工作機械サービス」「イベント＆ワークショップ」「Fabアイテム販売」の4領域から成りますが、拠点ごとにどの領域に力を入れるか比重が変わります。","comment_id":"43","x":6.308539,"y":10.506381,"p":0.6810050650487958},{"arg_id":"A44_0","argument":"台湾のFabCafe Taipeiではバリスタチャンピオンによるスペシャルティコーヒーが人気で、カフェ（飲食）分野が強みとなっています。","comment_id":"44","x":4.529343,"y":8.830679,"p":0.3655384752908202},{"arg_id":"A44_1","argument":"東京（FabCafe Tokyo）では売上構成比がおよそ飲食30%、デジタル工作機械15%、イベント＆ワークショップ55%とイベント系の比重が非常に高くなっています。","comment_id":"44","x":5.919034,"y":10.425374,"p":0.6810050650487958},{"arg_id":"A45_0","argument":"FabCafeの世界展開第1号となったのは台北（FabCafe Taipei）で、渋谷に次ぐ2店舗目として2013年にオープンしました。","comment_id":"45","x":4.054285,"y":8.722388,"p":1},{"arg_id":"A45_1","argument":"東京・台北に続いて、2014年3月27日にはヨーロッパ初となるFabCafe Barcelona（スペイン・バルセロナ）がクリエイター向けコワーキング施設MOB内に正式オープンしています。","comment_id":"45","x":4.079394,"y":8.695438,"p":1},{"arg_id":"A47_0","argument":"2014年にはスペインでFabCafe Barcelonaに加え、同国シッチェスにもFabCafeがオープンし、さらに翌2015年にはタイ・バンコクにも開設されるなど、グローバル展開が加速しました。","comment_id":"47","x":3.8415902,"y":8.582649,"p":0.6748449156485274},{"arg_id":"A48_0","argument":"2016年時点でもFabCafeネットワークは拡大を続け、フランスのストラスブールやメキシコのモンテレイで新拠点の準備が進められていました。このようにして毎年のように新たな都市へFabCafeが誕生しています。","comment_id":"48","x":4.0890446,"y":8.218091,"p":0.4466531506375286},{"arg_id":"A49_0","argument":"メキシコでは、首都メキシコシティにあるアート×サイエンスの実験的ラボ「Membrana Lab」の中にFabCafe Mexico Cityが2022年2月に誕生しました。バイオやXR（Extended Reality）の実験も行う先進的な施設内に設けられ、異分野のクリエイターをつなぐハブとなっています。","comment_id":"49","x":4.8089447,"y":9.334946,"p":0},{"arg_id":"A51_0","argument":"日本国内にもFabCafeは次々と展開しました。","comment_id":"51","x":3.956156,"y":8.379479,"p":0.6748449156485274},{"arg_id":"A55_0","argument":"2017年6月9日には京都・五条エリアにFabCafe Kyotoがオープンしました。築約120年の町家をリノベーションし、クリエイティブラウンジ「MTRL KYOTO」の1階に併設された形で、東京・飛騨に続く国内3店舗目となりました。","comment_id":"55","x":3.8502135,"y":9.551586,"p":1},{"arg_id":"A60_0","argument":"FabCafe Fujiの店内にはカフェのほか、アート・建築・テキスタイルに関する図書コーナーやリソグラフ印刷を楽しめるスペース、コワーキングスペースが併設されています。","comment_id":"60","x":5.7471147,"y":10.025488,"p":0},{"arg_id":"A62_0","argument":"2025年春には大阪・南森町にFabCafe Osakaがオープン予定です。東京・京都・飛騨・名古屋・富士吉田に続く国内6拠点目で、水の都・大阪らしく蒸留器を使って新たなカルチャーを育む場になる構想が発表されています。","comment_id":"62","x":3.6574008,"y":9.468499,"p":0.7855117661848798},{"arg_id":"A63_0","argument":"FabCafe Osakaでは、蒸留技術を活用した実験的プロジェクト（例えば地元の名産や素材を使ったクラフトスピリッツづくり等）が計画されており、都市とローカル文化が交錯する大阪ならではのFabCafeが期待されています。","comment_id":"63","x":4.082002,"y":9.718275,"p":1},{"arg_id":"A85_0","argument":"FabCafeは企業の新規事業開発やR\u0026Dにも活用されています。2023年にはFabCafe Tokyoにて、新規事業担当者向けに「共創現場のフィールドスタディ」を行うトークイベントが開催され、FabCafeの共創現場を体験的に学ぶ機会を提供しました。","comment_id":"85","x":4.993331,"y":9.428293,"p":0.8060871985428605},{"arg_id":"A89_0","argument":"FabCafe各拠点は、それぞれ地域の文化や産業資源を活かしたユニークなプロジェクトを展開しています。","comment_id":"89","x":6.3263516,"y":9.844365,"p":0},{"arg_id":"A91_0","argument":"FabCafe MTRL（素材リサーチ拠点）は、FabCafeから派生した取り組みで、素材メーカーとクリエイターの共創を支援するグローバルプラットフォームです。","comment_id":"91","x":4.8969603,"y":9.97718,"p":0},{"arg_id":"A91_1","argument":"東京や京都のFabCafeにはMTRLコーナーが併設され、最新素材の展示・実験を通じて新規プロジェクト創出が図られています。","comment_id":"91","x":4.6667085,"y":9.7191725,"p":0},{"arg_id":"A92_0","argument":"クリエイターと企業の共創事例としては、海洋テクノロジーベンチャーのEverblue TechnologiesとFabCafeが共同で進めたAIデザイン無人艇「ADAM」の開発プロジェクトがあります。これはFabCafeコミュニティのデザイナーやエンジニアが参画し、海洋課題に挑んだ例で、異分野連携によるイノベーション創出のモデルケースとなりました。","comment_id":"92","x":5.828204,"y":9.14009,"p":0},{"arg_id":"A93_0","argument":"FabCafeでは、最新テクノロジーとクリエイティブの交差点を探るイベントも盛んです。例えば、バイオテクノロジーとデザインの融合を探求するワークショップや、XR技術を用いた新しい表現を試す実験イベントなどが各地で行われています。","comment_id":"93","x":5.971054,"y":9.591404,"p":0.7084527133433565},{"arg_id":"A94_0","argument":"2024年11月には、ロフトワーク・FabCafe・千葉工業大学の3者による初の産学国際連携プロジェクト「Project Apophis」（小惑星探査プロジェクト）が始動しました。2029年に地球に最接近する小惑星アポフィスに探査機を送る計画で、企業・技術・資本・新しい才能や想像力を結集し宇宙領域のビジネス機会を探る壮大なプロジェクトです。","comment_id":"94","x":4.615359,"y":10.503102,"p":0},{"arg_id":"A95_0","argument":"Project Apophisでは、宇宙・非宇宙の垣根を超えた共創コンソーシアムを形成し、研究者と様々なプロフェッショナルやクリエイターをつなげるコミュニティづくりも目指されています。FabCafeはこのプロジェクトで、異分野の専門家が集う物理的・創造的ハブとして機能し、従来にないコラボレーションを実現しています。","comment_id":"95","x":5.833012,"y":9.290908,"p":0},{"arg_id":"A98_0","argument":"FabCafeはその空間自体が人と人、人と技術をつなぐ実験装置のような役割を果たしています。例えば店内の大型レーザーカッターや3Dプリンターを通じて生まれたプロジェクトが、そのままスタートアップの事業アイデアに発展したケースもあります（FabCafe Tokyo発のプロジェクトが起業につながった事例も複数存在）。","comment_id":"98","x":5.3071065,"y":9.4261675,"p":1},{"arg_id":"A99_0","argument":"FabCafeでは、毎年バレンタイン時期にレーザーカッターでオリジナルチョコレートを作るイベントなど、一般の人が最先端のデジタル工作を楽しめる企画も人気です。","comment_id":"99","x":5.313685,"y":9.92438,"p":0},{"arg_id":"A100_1","argument":"スタートアップやメーカーが試作したガジェットをFabCafeの利用者に試してもらいフィードバックを得る、といったライブプロトタイピングの光景も日常的に見られます。","comment_id":"100","x":5.5387764,"y":9.420004,"p":0.8739101973461189},{"arg_id":"A102_1","argument":"都市型のイノベーション拠点や地方創生のハブを作ろうとする動きにおいて、FabCafeの事例がしばしば引き合いに出され、そのナレッジが共有されています。","comment_id":"102","x":6.1603055,"y":8.909962,"p":1}]},{"cluster":"クリエイティブなものづくりのプラットフォーム","cluster_id":"4","takeaways":"FabCafeは、単なる飲食店ではなく、クリエイターをつなぐグローバルなプラットフォームとしての役割を果たしています。2011年から始まったこのムーブメントは、世界中に広がり、地域のイノベーションを促進する場となっています。FabCafeは、クリエイターが自らの創造力を発揮し、社会に新たな価値を提供することを目指しており、その理念は「What do you Fab?」という合言葉に象徴されています。\n\nまた、FabCafeは「FAB RACERS CUP」などの共創プロジェクトを通じて、遊びを通じた学びの機会を提供し、企業との連携を深めています。さらに、YouFab Global Creative Awardsを主催し、クリエイター同士の交流や社会課題への取り組みを促進しています。これらの活動を通じて、FabCafeはクリエイティブコミュニティをビジネスに結びつける成功モデルとして注目されています。","arguments":[{"arg_id":"A1_0","argument":"「FAB」という言葉には、大量生産や市場論理に縛られないものづくり（Fabrication）と「愉快で素晴らしい」（Fabulous）の意味が込められ、2011年時点で世界20カ国50カ所以上に広がるムーブメントでした。","comment_id":"1","x":5.38874,"y":8.326392,"p":1},{"arg_id":"A16_0","argument":"FabCafeは単なる飲食店ではなく、「世界中のクリエイターをつなぐ新しいプラットフォーム」であり「グローバルなクリエイティブコミュニティ」でもあるという高尚なビジョンを掲げています。","comment_id":"16","x":6.7030206,"y":8.405611,"p":1},{"arg_id":"A21_0","argument":"FabCafeのコミュニティがグローバルに広がることで企業への提供価値も高まり、各地のFabCafeネットワークを通じてさらなる付加価値を生み出せると考えています。","comment_id":"21","x":6.5841656,"y":8.397475,"p":1},{"arg_id":"A24_0","argument":"つまり、世の中に埋もれているマニアックな「面白い」テーマを発掘し、クリエイターの力で社会に開いていく――それがFabCafeの存在意義だと定義されています。","comment_id":"24","x":5.8763013,"y":8.514863,"p":0.9515614829201564},{"arg_id":"A27_0","argument":"FabCafeの合言葉「What do you Fab?」には、一人ひとりが自分の創造力で何かを“Fab（つくる）”してみようというメッセージが込められており、この精神が全ての活動の根底にあります。","comment_id":"27","x":5.2272725,"y":8.379453,"p":1},{"arg_id":"A36_0","argument":"こうしてFabCafeの理念に共鳴する世界中の仲間が有機的に増えていき、グローバルネットワークが形成されてきました​。その結果、FabCafeネットワークはフランチャイズでもNPOでもない、自立型のグローバル・クリエイティブコミュニティへと発展しています。","comment_id":"36","x":6.358848,"y":8.191991,"p":0.9243893226510576},{"arg_id":"A38_0","argument":"2014年には、FabCafeの活動（都市づくり・地域づくり・コミュニティづくりへの貢献）が評価され、グッドデザイン賞「地域・コミュニティづくり」分野を受賞しました。","comment_id":"38","x":7.159084,"y":8.197534,"p":0.9265846328895276},{"arg_id":"A42_0","argument":"グローバル展開の目的は、世界中のクリエイター同士をネットワークし、新たな価値創造の機会を生み出すことにあります。","comment_id":"42","x":6.67948,"y":8.299182,"p":1},{"arg_id":"A42_1","argument":"各地のFabCafeが連携することで、ローカルなイノベーションをグローバルに波及させるプラットフォームとなっています。","comment_id":"42","x":6.2286253,"y":8.459833,"p":1},{"arg_id":"A46_1","argument":"MOB内で生まれるプロジェクトやアイデアとFabCafeの設備・ネットワークが融合することで、新たなコラボレーションが生まれています。","comment_id":"46","x":6.0302634,"y":8.642528,"p":1},{"arg_id":"A58_1","argument":"また、世界中のFabCafeネットワークとロフトワークのグローバルネットワークを活用し、地域内に留まらない国内外との共創も促進しています。","comment_id":"58","x":6.387973,"y":7.942331,"p":0},{"arg_id":"A78_0","argument":"こうした実績が評価され、2014年にはグッドデザイン賞（地域・コミュニティづくり）を受賞し、FabCafeはデザインとコミュニティ両面で社会から認知される存在となりました。","comment_id":"78","x":7.089556,"y":8.240803,"p":1},{"arg_id":"A80_0","argument":"2014年から始まった「FAB RACERS CUP」は、FabCafe発の代表的な共創プロジェクトです。ミニ四駆やドローンを題材に、大人が真剣に遊びながらエンジニアリングやデザインを学ぶ競技大会・ワークショップシリーズで、年に1回の大会に向けて数ヶ月にわたりハッカソンや勉強会が開催されます。","comment_id":"80","x":5.5996118,"y":7.696999,"p":1},{"arg_id":"A81_0","argument":"FAB RACERSは、未就学児から50代まで幅広い参加者層を持ち、現在300人を超えるコミュニティへと成長しました。","comment_id":"81","x":5.605125,"y":7.73722,"p":1},{"arg_id":"A82_0","argument":"FAB RACERSには2つのミッションがあります。一つは「遊びを通じて学びを提供すること」で、単なるレース大会ではなくIoTやAIを含む最新技術を学べるハッカソンやワークショップを組み合わせています。","comment_id":"82","x":5.547127,"y":7.669625,"p":1},{"arg_id":"A83_0","argument":"FAB RACERSのように、FabCafe発のコミュニティが企業との共創プラットフォームに発展した例は他にもあります。例えば、FabCafe Nagoyaでは地元企業と連携した「名古屋共創会議」を開催し、中小企業のデザイン経営に関する具体事例共有や実践的議論の場を提供しています。","comment_id":"83","x":6.0356317,"y":8.273372,"p":0.9515614829201564},{"arg_id":"A84_0","argument":"FabCafe Nagoyaの共創会議では、参加企業の担当者同士がコミュニティとしてつながり合い、従来にはない発想や手法で自社の未来像を描くことにチャレンジしています。","comment_id":"84","x":6.207339,"y":8.853326,"p":1},{"arg_id":"A87_0","argument":"FabCafeはまた、クリエイターと社会をつなぐグローバルなプラットフォームとして2012年から「YouFab Global Creative Awards」を主催しています。世界中からデジタルものづくりやメディアアートの作品応募を募り、優れた作品を表彰するアワードで、クリエイター同士の交流や社会との接点を生み出しています。","comment_id":"87","x":6.8720303,"y":7.977897,"p":0},{"arg_id":"A88_0","argument":"YouFab受賞作品には、社会課題にクリエイティブに取り組むものも多く、FabCafeはアワードを通じてグローバルにクリエイターコミュニティを可視化するとともに、企業や自治体が注目すべき先端事例を提示する役割も果たしています。","comment_id":"88","x":6.9961157,"y":8.079385,"p":1},{"arg_id":"A96_0","argument":"FabCafe Bangkokでは、国連機関ユネスコと協働し、東北タイの若い女性たちにエンパワーメントの機会を提供するプログラムを展開しました。People・Planet・Profitのトリプルボトムラインを重視したこのイニシアチブは、FabCafeを拠点に地域課題の解決と女性の社会参画をクリエイティブに支援する試みです。","comment_id":"96","x":7.015552,"y":8.504263,"p":0.6815568103064477},{"arg_id":"A99_1","argument":"こうした身近で遊び心あるイベントを通じて、“Fab”の概念がクリエイター以外にも広がりました。","comment_id":"99","x":5.4176984,"y":8.137996,"p":0.6627772368649452},{"arg_id":"A102_0","argument":"FabCafeの成功は、クリエイティブコミュニティをビジネスにつなげるモデルケースとして国内外で注目されています。","comment_id":"102","x":6.9539523,"y":8.739324,"p":0.7957938913218576}]},{"cluster":"クリエイティブなものづくりのプラットフォーム","cluster_id":"2","takeaways":"FabCafeは、ロフトワークの諏訪光洋氏や福田敏也氏、林千晶氏などのプロデュースのもと、地域の伝統や資源を活かしたものづくりの拠点として展開されています。岐阜県飛騨市のFabCafe Hidaは、古民家をリノベーションし、クリエイター向けの滞在施設として機能し、地域産業の再生を目指しています。また、名古屋や富士吉田にも拠点があり、それぞれ地域の特性を活かしたクリエイティブな活動が行われています。\n\n特にFabCafe Hidaでは、未利用の国産材を使用した「森のクレヨン」の開発など、環境問題への取り組みも行われており、地域の持続可能性を意識したプロダクトが生まれています。これらの拠点は、地域のクリエイターや企業との連携を通じて新たな価値を創造し、地域の未来づくりに貢献する場となっています。","arguments":[{"arg_id":"A6_0","argument":"FabCafe立ち上げには、ロフトワーク代表の諏訪光洋氏、クリエイター福田敏也氏、ロフトワーク共同創業者の林千晶氏がプロデュース陣として携わり、空間デザインは成瀬友梨氏・猪熊純氏（建築家）が担当、FabLab Japan創設者の田中浩也氏もサポーターに名を連ねました。","comment_id":"6","x":4.5638976,"y":11.238465,"p":0},{"arg_id":"A51_1","argument":"2016年春には岐阜県飛騨市古川町に、築100年以上の酒蔵兼木工所だった古民家をリノベーションしたFabCafe Hidaがオープンしています。","comment_id":"51","x":3.4721549,"y":10.346989,"p":0.5909082495865262},{"arg_id":"A52_0","argument":"FabCafe Hidaは、飛騨市と民間（株式会社飛騨の森でクマは踊る＝ヒダクマ）およびロフトワークの官民共同事業として設立されました。","comment_id":"52","x":3.1710248,"y":10.650443,"p":1},{"arg_id":"A52_1","argument":"豊かな森林資源と伝統木工技術を持つ飛騨ならではの文脈で、地域産業の再生とものづくりの革新を目指す拠点です。","comment_id":"52","x":3.5800395,"y":10.845503,"p":0.8512077245305986},{"arg_id":"A53_0","argument":"飛騨古川のFabCafe Hidaは、宿泊機能を備えたクリエイター向け滞在施設としての側面も持ちます。","comment_id":"53","x":3.250331,"y":10.410596,"p":1},{"arg_id":"A53_1","argument":"クリエイターやデザイナー、建築家が滞在しながら広葉樹林の活用や伝統的な組木技術について学び、新技術との融合を探る「創造的な実験の場」として機能しています。","comment_id":"53","x":3.7360287,"y":11.102703,"p":0.649709238335217},{"arg_id":"A54_0","argument":"FabCafe Hidaでは、企業研修や合宿プログラムの提供も行っており、森を起点にしたものづくりのプロセスを企業の人材育成やプロジェクト創出に役立てるユニークな取り組みを展開しています。","comment_id":"54","x":3.1005466,"y":10.722265,"p":0.9820544686667596},{"arg_id":"A56_0","argument":"FabCafe Kyotoは鴨川近くの文化的なエリアに位置し、「ヒト・モノ・コトが混じりあい化学反応を起こすボーダレスでオープンな場」をコンセプトに掲げています。和の伝統とデジタルものづくりが交差する拠点として、地元クリエイターや学生、観光客まで幅広い層が集う場となっています。","comment_id":"56","x":3.9335775,"y":9.988896,"p":1},{"arg_id":"A57_0","argument":"2020年秋には名古屋・久屋大通公園内にFabCafe Nagoyaが開設されました。OKB総研（大垣共立銀行グループ）とロフトワークの協働によるプロジェクトで、地元企業と共に新たな価値を創造し地域の未来づくりに貢献する発信・交流拠点として位置づけられています。","comment_id":"57","x":4.249095,"y":10.3230915,"p":0.7562964180713315},{"arg_id":"A58_0","argument":"FabCafe Nagoyaは、東海地域有数のものづくり産業や多様な素材（繊維・土・木・食品など）の集積を背景に、それらに関わる人々をつなげることで新たな化学反応を起こすことを目指しています。","comment_id":"58","x":4.1127963,"y":10.122498,"p":1},{"arg_id":"A59_0","argument":"2022年11月6日、山梨県富士吉田市に国内5拠点目（世界14拠点目）となるFabCafe Fujiがオープンしました。富士山麓の織物産地・富士吉田の伝統産業とクリエイターの共創を目指す拠点であり、地域資源とクリエイティブを掛け合わせた新しい挑戦となっています。","comment_id":"59","x":3.5440738,"y":9.844732,"p":0.7431532085287812},{"arg_id":"A60_1","argument":"富士吉田という1000年以上の織物の歴史を持つ町に根ざし、「テキスタイル」を中心テーマにクリエイティブコミュニティを育むことを目指しています。","comment_id":"60","x":3.8543942,"y":10.73234,"p":0.8512077245305986},{"arg_id":"A61_0","argument":"富士吉田では既にクリエイターやアーティストと地元機（はた）屋が協業して新たな織物価値を創造する動きがあり、FabCafe Fujiもそうした伝統と革新の交差点として機能しようとしています。地元住民や織物業界関係者、自治体とも連携しながら、展示会やイベントを通じて地域に新風を吹き込んでいます。","comment_id":"61","x":3.9144237,"y":10.620723,"p":0.6269804047986542},{"arg_id":"A81_1","argument":"自動車・バイクデザイン界を代表するプロも参加しており、トヨタのコンセプトカーを手掛けた根津孝太氏や、本田のヒットバイクをデザインしたやまざきたかゆき氏が大会委員長・副委員長を務めています。","comment_id":"81","x":4.31057,"y":10.996434,"p":0},{"arg_id":"A89_1","argument":"例えばFabCafe Hidaでは、未利用の国産材を100%使った「森のクレヨン」という木製クレヨンを開発し、日本の森林問題にクリエイティブにアプローチするプロダクトを生み出しました。","comment_id":"89","x":3.3401687,"y":10.995172,"p":0.8934837700718604},{"arg_id":"A90_0","argument":"FabCafe Hida発の「森のクレヨン」は、家具や建材に使われない端材や間伐材をクレヨンにアップサイクルしたもので、林野庁との協働プロジェクトとして森の持続可能性への意識啓発にも貢献しています。","comment_id":"90","x":3.2705276,"y":10.915336,"p":0.9820544686667596}]},{"cluster":"クリエイティブなものづくりのプラットフォーム","cluster_id":"0","takeaways":"FabCafeは、ロフトワークの企業ミッション「クリエイティブの流通」を実現するために設立されました。ロフトワークは、クリエイターのネットワークを活用し、リアルな場での共創を目指して初めての飲食店舗運営に挑戦しました。FabCafeは、オープンなカフェ形式で誰でも立ち寄れる場所を提供し、地域の特性を活かした自主運営を行っています。\n\nロフトワークは、FabCafeを通じてクリエイターと企業・社会をつなぐ共創プラットフォームを構築し、多様なステークホルダーとの協業を促進しています。これにより、飲食店運営のノウハウやコミュニティマネジメントの経験を蓄積し、組織全体のクリエイティブサービス提供力を向上させています。FabCafeは、単なる事業ではなく、ロフトワークの理念を実証し続ける「実験の場」として機能しています。","arguments":[{"arg_id":"A7_0","argument":"FabCafeが誕生した背景には、運営母体ロフトワークの企業ミッションである「クリエイティブの流通」をリアルな場で実現したいという思いもありました。ロフトワークは2000年の創業以来クリエイターのネットワークを築きウェブで展開してきましたが、FabCafeは初めての飲食店舗運営への挑戦でした。","comment_id":"7","x":5.5869803,"y":11.763955,"p":0.5468560181335214},{"arg_id":"A8_0","argument":"ロフトワークは、ネット上のクリエイターコミュニティで蓄積した共創のノウハウをリアル空間に持ち込み、誰もが立ち寄れるカフェというオープンな形で新たな創造の化学反応を起こすことを目指してFabCafeを開設しました。","comment_id":"8","x":5.6475053,"y":12.535707,"p":1},{"arg_id":"A35_0","argument":"誰でも簡単にFabCafeをオープンできるわけではなく、実現に至るのは一部です。新拠点開設にあたってはいくつかの条件や十分な話し合いを経てから進められており、コンセプトへの深い共感と持続可能性が求められます。","comment_id":"35","x":6.1223288,"y":11.151269,"p":1},{"arg_id":"A37_0","argument":"FabCafeは「カフェ」という親しみやすいフォーマットを採りながらも、フランチャイズ展開は行っていません。各地のFabCafeはロフトワークやパートナー企業との協働で運営され、共通の理念を持ちながらも地域の特徴を活かした自主運営がなされています。","comment_id":"37","x":6.030316,"y":11.406096,"p":0},{"arg_id":"A64_0","argument":"Loftwork（ロフトワーク）はFabCafeの運営母体であり、FabCafeを通じて自身のミッションである「オープンコラボレーションによる価値創造」を体現しています。","comment_id":"64","x":5.400253,"y":12.508872,"p":0.831201287679764},{"arg_id":"A65_0","argument":"株式会社ロフトワークは、ウェブやコンテンツ、コミュニケーション、空間などをオープンコラボレーションでデザインするクリエイティブ・カンパニーであり、FabCafeのほか素材共創プラットフォームのMTRLや共創プログラムのAWRDなどを運営しています。","comment_id":"65","x":5.6192503,"y":12.330924,"p":1},{"arg_id":"A66_0","argument":"ロフトワークの企業ミッションは創業当初から「クリエイティブの流通」を実現することでした。15年以上にわたりクリエイター向けポータルサイトを運営し、多様なクリエイティブ案件を手掛けてきたロフトワークにとって、FabCafeはリアルな場でクリエイターコミュニティを運営する新たな挑戦でした。","comment_id":"66","x":5.547208,"y":12.251964,"p":1},{"arg_id":"A67_0","argument":"FabCafeの運営を通じてロフトワークは、自社が培ってきた世界有数のクリエイティブコミュニティとプロジェクトマネジメント力を活かし、人と企業と社会に継続的に向き合い価値を生み続けるビジネスエコシステムの構築を目指しています。","comment_id":"67","x":6.102068,"y":12.063343,"p":1},{"arg_id":"A68_0","argument":"ロフトワークがFabCafeを運営する理由の一つは、オープンイノベーションの場を自ら持つことで、クライアントワークに留まらないダイナミックな価値創出を可能にするためです。","comment_id":"68","x":5.4608917,"y":12.570152,"p":1},{"arg_id":"A69_0","argument":"FabCafeはロフトワークにとって、クリエイターと企業・社会をつなぐ共創プラットフォームとして機能します。これによりロフトワークは多様なステークホルダーを巻き込んだプロジェクトを推進しやすくなり、クリエイティブ企業としての提供価値を拡張しています。","comment_id":"69","x":6.000554,"y":11.994624,"p":1},{"arg_id":"A70_0","argument":"実際、ロフトワーク/FabCafeはこれまでに大企業から自治体、スタートアップ、大学、研究機関、国際機関まで様々なパートナーと協業してプロジェクトを生み出してきました。","comment_id":"70","x":5.106959,"y":12.037129,"p":0},{"arg_id":"A70_1","argument":"例えば、UNDP（国連開発計画）や在日イスラエル大使館、慶應義塾大学、沖縄科学技術大学院大学（OIST）、講談社、TSUTAYA（CCC）などとの共創実績があります。","comment_id":"70","x":4.8488917,"y":11.6997385,"p":0},{"arg_id":"A71_0","argument":"ロフトワークの社員やFabCafeのチームメンバーは多彩なバックグラウンドを持っており、それぞれがテーマを掲げてコミュニティを形成しています。","comment_id":"71","x":5.909837,"y":12.50494,"p":0.7826593185752891},{"arg_id":"A72_0","argument":"FabCafeを運営することでロフトワーク自身も新たな知見を得ています。飲食店運営のノウハウやコミュニティマネジメントの経験は、他のクライアントワークや自社プロジェクトにも活かされ、組織としての進化につながっています。","comment_id":"72","x":6.006267,"y":12.051509,"p":1},{"arg_id":"A73_0","argument":"ロフトワークにとってFabCafeは、単に事業の一つではなく、自社の理念を実証しアップデートし続ける「実験の場」です。その成功と失敗の両方から得られた学びが、ロフトワーク全体のクリエイティブサービス提供力を高めています。","comment_id":"73","x":5.6043005,"y":12.192393,"p":1},{"arg_id":"A74_0","argument":"FabCafeはロフトワークに属しつつも、FabCafe LLP（有限責任事業組合）という形でグローバル展開の各拠点パートナーと連携する組織体制を取っています。これにより各FabCafeはロフトワークのネットワークとサポートを受けながら自主性も持って運営されています。","comment_id":"74","x":5.7036657,"y":11.706572,"p":0.4997390512636932},{"arg_id":"A75_0","argument":"ロフトワーク台湾の創業者でFabCafe TaipeiオーナーでもあるTim Wong氏は「領域を超え、複雑な課題へのソリューションを発見するという私たちのミッションにおいて、FabCafeは重要な役割を果たしている」と述べています。このように各地域のパートナーにとってもFabCafeはミッション達成の手段となっています。","comment_id":"75","x":5.4684615,"y":12.122917,"p":1},{"arg_id":"A76_0","argument":"FabCafeは、その空間とコミュニティ自体がロフトワークにとっての重要なアセット（資産）です。多様な人材ネットワークや知見が集積する場であり、ここから生まれるプロジェクトやプロダクトが数多くあります。","comment_id":"76","x":6.532031,"y":11.797747,"p":0},{"arg_id":"A86_0","argument":"ロフトワークの共創の知見を共有するこのフィールドスタディでは、FabCafeを舞台に実際のコミュニティドリブンなプロジェクト事例を紹介し、参加者が自社で共創を実践するためのヒントを持ち帰っています。","comment_id":"86","x":5.870489,"y":12.535845,"p":0.9270781846348364}]},{"cluster":"クリエイティブなコラボレーションの場","cluster_id":"3","takeaways":"FabCafeは、多様なクリエイターが集まる「場づくり」を重視し、コミュニティの育成を通じてビジネスを成立させています。利益を追求する一方で、クリエイター同士のつながりや新しいプロジェクトの創出を促進することが基本的なビジネスモデルです。オープンな場を提供し、異なるバックグラウンドを持つ人々を結びつけることで、偶発的な出会いや新たなアイデアが生まれる環境を整えています。\n\nまた、FabCafeは地域ごとの特性を活かし、各拠点で独自のクリエイターコミュニティを形成しています。イベントやワークショップを通じて、メンバーは新しい知識やスキルを得る機会を持ち、企業とのコラボレーションを通じて新たなビジネス機会を探ることも行っています。これにより、FabCafeは単なる商業空間を超え、ソーシャルイノベーションのプラットフォームとしても機能しています。","arguments":[{"arg_id":"A14_0","argument":"FabCafeは積極的にワークショップを企画・実施し、メディアの取材も受け入れ、企業とのコラボレーションも仕掛けるなど、多方面に情報発信と巻き込みを行うことで次第にビジネスとして成立するようになりました。","comment_id":"14","x":7.0501366,"y":9.728131,"p":0.9431439287739},{"arg_id":"A18_0","argument":"FabCafeが最も大切にしているのは、多様な人々が集まるための「場づくり」です。利益は継続のために必要ですが、利益最優先ではなく、まずクリエイターが集うコミュニティを育むことを重視しています。","comment_id":"18","x":7.552373,"y":9.629448,"p":0.5362157841053511},{"arg_id":"A20_0","argument":"このようにして多種多様なクリエイターからなるコミュニティをつくり上げ、そのコミュニティにアクセスしたい企業にサービスを提供する――これがFabCafeの基本的なビジネスモデルです。","comment_id":"20","x":7.185199,"y":9.121957,"p":0},{"arg_id":"A23_0","argument":"FabCafeのミッションは、「プロジェクトはもちろん、まだプロジェクト化されていないイシュー（社会課題）を見つけ出し、クリエイターと共にその意味を考え、社会とつなげること」です。","comment_id":"23","x":7.411161,"y":8.7828045,"p":0.880061454272925},{"arg_id":"A28_0","argument":"FabCafeでは、スタッフ自身がコミュニティを刺激する存在になることが求められます。コミュニティ維持のための場（施設）を提供するだけでなく、最新のトレンドに常にアンテナを張って新しい刺激をもたらすことも重視しています。","comment_id":"28","x":7.4160075,"y":9.868463,"p":0},{"arg_id":"A29_0","argument":"FabCafeが大事にするバリューの一つに「Openness（開放性）」があります。誰もがふらっと立ち寄れるオープンな場であることで偶発的な出会いが生まれると考えているからです。","comment_id":"29","x":7.1696525,"y":10.390805,"p":0},{"arg_id":"A30_0","argument":"また、「Connect（つなげること）」も重要です。ただ場所があるだけでは人は集まりません。異なるバックグラウンドを持つ人同士をつなぎ、新しい“血”をコミュニティに入れ込むためのオープンな機会を設けるよう努めています。","comment_id":"30","x":8.014337,"y":9.419206,"p":1},{"arg_id":"A31_0","argument":"コミュニティ内にメンバーがコミットできる具体的な「Project（プロジェクト）」があることも重視されています。そうした活動があれば、興味のある人が自然と集まり、時にはメンバー発信でプロジェクトが始まることもあるからです。","comment_id":"31","x":8.078076,"y":9.496119,"p":1},{"arg_id":"A32_0","argument":"さらに、コミュニティ運営を持続可能にするには「Business（ビジネス）」の要素も欠かせません。プロジェクトと企業をつなげて予算を確保したり、予算のついたプロジェクトをコミュニティで回したりするサイクルを作り、経済的にも自立できるモデルを追求しています。","comment_id":"32","x":8.140061,"y":9.322879,"p":1},{"arg_id":"A33_0","argument":"FabCafeのネットワーク拡大に際しては、お金よりも各拠点の運営者となる人のパッションやバックグラウンド、そしてその人が持つコミュニティを重視しています。","comment_id":"33","x":7.6712365,"y":9.424171,"p":0.5362157841053511},{"arg_id":"A41_0","argument":"FabCafeはデジタルファブリケーションを活動の出発点としつつ、各拠点それぞれが多様なテクノロジーや文化を巻き込み、その土地ならではのクリエイターコミュニティを形成しています。","comment_id":"41","x":6.6298194,"y":9.668021,"p":0},{"arg_id":"A46_0","argument":"バルセロナ店は300人以上のメンバーを抱えるコワーキングスペースと併設されており、地域のクリエイターコミュニティとの強固な結びつきを特徴としています。","comment_id":"46","x":7.184216,"y":8.935387,"p":1},{"arg_id":"A50_0","argument":"FabCafe Mexico Cityは、アートとテクノロジー、サイエンスを横断するプロジェクトを推進する拠点で、創業メンバー自身がバイオメディアアーティストやキュレーターといった経歴を持ち、アートのエコシステムの持続可能性を探求する活動を展開しています。","comment_id":"50","x":6.4031935,"y":9.909152,"p":0},{"arg_id":"A68_1","argument":"自社のプラットフォームとしてFabCafeを位置づけることで、新規事業創出やコミュニティデザインの実験を行いやすくしています。","comment_id":"68","x":6.605903,"y":9.032345,"p":0},{"arg_id":"A71_1","argument":"FabCafeというプラットフォームの上でメンバーが主体的に活動テーマを持ち、プロジェクトやイベントを通じてアウトプットするカルチャーが根付いています。","comment_id":"71","x":6.9814663,"y":9.694948,"p":1},{"arg_id":"A77_0","argument":"開業から数年で、FabCafeは延べ17万人以上の来訪者を迎え、そこで生まれた作品（プロジェクト）も1万点を超えました。また開催されたイベントは400件以上に上り、コミュニティ発の活動が活発に行われています。","comment_id":"77","x":6.923738,"y":9.318981,"p":1},{"arg_id":"A79_0","argument":"FabCafeでは常に何らかのイベントやワークショップが行われており、そのテーマはテクノロジー、アート、デザイン、サイエンス、ビジネスなど多岐にわたります。これによりコミュニティメンバーは新しい知識やスキルを得る機会に恵まれ、交流を通じて新プロジェクトの種が蒔かれています。","comment_id":"79","x":6.802301,"y":9.94399,"p":0},{"arg_id":"A82_1","argument":"もう一つは「企業とクリエイターを結びつけ新たなビジネス機会を探ること」で、仕事とは別の動機で集まったプロたちと企業が楽しく交流しながら新しい可能性を模索しています。","comment_id":"82","x":7.9306192,"y":9.143371,"p":0.6382961078368614},{"arg_id":"A84_1","argument":"FabCafeが仲介することで、異業種・異分野の出会いから新規事業のヒントが得られる場となっています。","comment_id":"84","x":6.93904,"y":10.340555,"p":0},{"arg_id":"A97_0","argument":"このバンコクのプロジェクトは、FabCafeが単なる商業空間にとどまらずソーシャルイノベーションのプラットフォームとしても活用されうることを示しました。地域の社会課題に対し、FabCafeコミュニティの若手クリエイターがデザイン思考で取り組み、具体的な成果や提言を生み出しています。","comment_id":"97","x":7.301835,"y":8.725416,"p":1},{"arg_id":"A100_0","argument":"開かれたコミュニティであるFabCafeは、新製品やサービスのユーザーテストの場としても活用されています。","comment_id":"100","x":6.7072067,"y":9.205253,"p":1},{"arg_id":"A101_0","argument":"情報発信においてもFabCafeは重要な拠点です。FabCafeの公式ウェブマガジンや各種SNSでは、各拠点で生まれたプロジェクト事例やイベントレポートが随時紹介され、それを見た企業やクリエイターが新たにコミュニティに参加するという好循環が生まれています。","comment_id":"101","x":7.0796456,"y":9.430778,"p":1}]}],"comments":{"0":{"comment":"FabCafeは、デジタル製造（FAB）のムーブメントを日常の場で体験できるようにするため、2012年に東京・渋谷で誕生した世界初の「レーザーカッターが主役」のものづくりカフェです。"},"1":{"comment":"「FAB」という言葉には、大量生産や市場論理に縛られないものづくり（Fabrication）と「愉快で素晴らしい」（Fabulous）の意味が込められ、2011年時点で世界20カ国50カ所以上に広がるムーブメントでした。"},"2":{"comment":"FabCafeは、このグローバルに広がる“ものづくり革命”の精神（FABスピリット）を日本でも楽しく美味しく分かりやすく伝える場として企画されました。"},"3":{"comment":"若者のエネルギーがあふれる渋谷という立地を選び、レーザーカッターをはじめ様々なデジタル工作機器を備えることで、多くの人がワクワクしながら新しいものづくりを楽しめる空間を目指しました。"},"4":{"comment":"FabCafe Tokyo（渋谷道玄坂）は2012年3月7日にオープンし、日本初のデジタルものづくりカフェとして話題を集めました。当初店舗面積40席のカフェスペースにレーザーカッターを据え、来店客が手軽にデジタル工作を体験できるようにしました。"},"5":{"comment":"FabCafeのコンセプトは「What do you Fab?（あなたは何をFabしますか？）」で、iPhoneケースやグリーティングカード、アクセサリー、椅子、照明、果ては家のモデルまで、様々なものをデジタル加工で“Fab”できることを示しました。"},"6":{"comment":"FabCafe立ち上げには、ロフトワーク代表の諏訪光洋氏、クリエイター福田敏也氏、ロフトワーク共同創業者の林千晶氏がプロデュース陣として携わり、空間デザインは成瀬友梨氏・猪熊純氏（建築家）が担当、FabLab Japan創設者の田中浩也氏もサポーターに名を連ねました。"},"7":{"comment":"FabCafeが誕生した背景には、運営母体ロフトワークの企業ミッションである「クリエイティブの流通」をリアルな場で実現したいという思いもありました。ロフトワークは2000年の創業以来クリエイターのネットワークを築きウェブで展開してきましたが、FabCafeは初めての飲食店舗運営への挑戦でした​。"},"8":{"comment":"ロフトワークは、ネット上のクリエイターコミュニティで蓄積した共創のノウハウをリアル空間に持ち込み、誰もが立ち寄れるカフェというオープンな形で新たな創造の化学反応を起こすことを目指してFabCafeを開設しました。"},"9":{"comment":"FabCafeは通常のカフェとは一線を画し、建築家・イラストレーター・エンジニア・アーティスト・研究者・ハッカー・アクティビスト・ドローンパイロットなど多彩なプロフェッショナルが集う「秘密基地」のような存在となりました。"},"10":{"comment":"異なる分野のクリエイター同士が交わることで日々化学反応が起こり、新たなプロジェクトが次々と生まれる実験の場――それがFabCafeの目指した世界観です。"},"11":{"comment":"FabCafeは誕生当初、コーヒーを淹れられるバリスタ不在時にはコーヒー提供を休止したり、工作機械を扱えるスタッフ不在時には「マシンメンテナンス中」の札を出すなど、手探りで運営を開始しました。"},"12":{"comment":"オープン直後は立地条件（渋谷・道玄坂上という一等地ではあるが人通りは限られる）もあって集客に苦労し、店内が閑散とする日も多く、スタッフ総出で街頭でチラシ配りをするところからスタートしました。"},"13":{"comment":"開店当初は赤字続きで、「店を続けること」自体が大きなチャレンジの連続でしたが、徐々にサービス改善やオペレーション改善を重ねていきました。"},"14":{"comment":"FabCafeは積極的にワークショップを企画・実施し、メディアの取材も受け入れ、企業とのコラボレーションも仕掛けるなど、多方面に情報発信と巻き込みを行うことで次第にビジネスとして成立するようになりました。"},"15":{"comment":"例えば、平日の昼間はノマドワーカーや学生が電源・WiFi完備のカフェ空間を利用し、夜や週末にはものづくり系のイベントやワークショップが頻繁に開催されるという具合に、時間帯によって様々な顔を持つ場所となっていきました。"},"16":{"comment":"FabCafeは単なる飲食店ではなく、「世界中のクリエイターをつなぐ新しいプラットフォーム」であり「グローバルなクリエイティブコミュニティ」でもあるという高尚なビジョンを掲げています。"},"17":{"comment":"そのためFabCafeの印象は人によって「美味しいコーヒーやおしゃれなスイーツが楽しめるカフェ」から「3Dプリンタがあるメイカースペース」「いつも外国人が集まっていてイベントをやっている場所」「グローバルなクリエイターコミュニティのハブ」まで様々ですが、どれもFabCafeの一面を表しています。"},"18":{"comment":"FabCafeが最も大切にしているのは、多様な人々が集まるための「場づくり」です。利益は継続のために必要ですが、利益最優先ではなく、まずクリエイターが集うコミュニティを育むことを重視しています。"},"19":{"comment":"カフェとしてお客様の回転率を上げる効率よりも、クリエイター（＝デザインしたりモノづくりできる人）を惹きつけるために最新のデジタル工作マシンを導入し、魅力的なイベントやワークショップを頻繁に開催する方針をとっています。"},"20":{"comment":"このようにして多種多様なクリエイターからなるコミュニティをつくり上げ、そのコミュニティにアクセスしたい企業にサービスを提供する――これがFabCafeの基本的なビジネスモデルです。"},"21":{"comment":"FabCafeのコミュニティがグローバルに広がることで企業への提供価値も高まり、各地のFabCafeネットワークを通じてさらなる付加価値を生み出せると考えています。"},"22":{"comment":"私たちは、クリエイターが集まり新しいものづくりのコラボレーションが起きる“装置”としてのFabCafeから、次世代のものづくりを変革するイノベーションが生まれるはずだと本気で信じています。"},"23":{"comment":"FabCafeのミッションは、「プロジェクトはもちろん、まだプロジェクト化されていないイシュー（社会課題）を見つけ出し、クリエイターと共にその意味を考え、社会とつなげること」です。"},"24":{"comment":"つまり、世の中に埋もれているマニアックな「面白い」テーマを発掘し、クリエイターの力で社会に開いていく――それがFabCafeの存在意義だと定義されています。"},"25":{"comment":"FabCafeは「実験の場」を自称しており、定型化されたプロジェクトだけでなく、実験段階のテーマやアイデアにも果敢に取り組みます。"},"26":{"comment":"少人数でニッチなワークショップの設計からスタートし、そこからムーブメントの起爆剤となるような仕掛けを生み出すこともFabCafeの使命の一部です​。"},"27":{"comment":"FabCafeの合言葉「What do you Fab?」には、一人ひとりが自分の創造力で何かを“Fab（つくる）”してみようというメッセージが込められており、この精神が全ての活動の根底にあります。"},"28":{"comment":"FabCafeでは、スタッフ自身がコミュニティを刺激する存在になることが求められます。コミュニティ維持のための場（施設）を提供するだけでなく、最新のトレンドに常にアンテナを張って新しい刺激をもたらすことも重視しています。"},"29":{"comment":"FabCafeが大事にするバリューの一つに「Openness（開放性）」があります。誰もがふらっと立ち寄れるオープンな場であることで偶発的な出会いが生まれると考えているからです。"},"30":{"comment":"また、「Connect（つなげること）」も重要です。ただ場所があるだけでは人は集まりません。異なるバックグラウンドを持つ人同士をつなぎ、新しい“血”をコミュニティに入れ込むためのオープンな機会を設けるよう努めています。"},"31":{"comment":"コミュニティ内にメンバーがコミットできる具体的な「Project（プロジェクト）」があることも重視されています。そうした活動があれば、興味のある人が自然と集まり、時にはメンバー発信でプロジェクトが始まることもあるからです。"},"32":{"comment":"さらに、コミュニティ運営を持続可能にするには「Business（ビジネス）」の要素も欠かせません。プロジェクトと企業をつなげて予算を確保したり、予算のついたプロジェクトをコミュニティで回したりするサイクルを作り、経済的にも自立できるモデルを追求しています。"},"33":{"comment":"FabCafeのネットワーク拡大に際しては、お金よりも各拠点の運営者となる人のパッションやバックグラウンド、そしてその人が持つコミュニティを重視しています。"},"34":{"comment":"実際、FabCafeの公式サイトには「Open a FabCafe in your city and join us!（あなたの街にFabCafeを作ろう、仲間に加わろう！）」という驚くほど気軽な問い合わせフォームが設置されており、月に数件は世界中から「自分の街でFabCafeを開きたい」というメールが届くほどです。"},"35":{"comment":"もっとも、誰でも簡単にFabCafeをオープンできるわけではなく、実現に至るのは一部です。新拠点開設にあたってはいくつかの条件や十分な話し合いを経てから進められており、コンセプトへの深い共感と持続可能性が求められます。"},"36":{"comment":"こうしてFabCafeの理念に共鳴する世界中の仲間が有機的に増えていき、グローバルネットワークが形成されてきました​。その結果、FabCafeネットワークはフランチャイズでもNPOでもない、自立型のグローバル・クリエイティブコミュニティへと発展しています。"},"37":{"comment":"FabCafeは「カフェ」という親しみやすいフォーマットを採りながらも、フランチャイズ展開は行っていません​。各地のFabCafeはロフトワークやパートナー企業との協働で運営され、共通の理念を持ちながらも地域の特徴を活かした自主運営がなされています。"},"38":{"comment":"2014年には、FabCafeの活動（都市づくり・地域づくり・コミュニティづくりへの貢献）が評価され、グッドデザイン賞「地域・コミュニティづくり」分野を受賞しました。"},"39":{"comment":"FabCafeが掲げる「マニアックな『面白い』を世の中にひらいていく」というビジョンは、言い換えれば専門領域の知見をオープンに共有し社会課題解決につなげることです。このビジョンのもと、各FabCafeでは先端テクノロジーや文化と地域コミュニティを交差させるユニークな取り組みが実践されています。"},"40":{"comment":"2012年の東京（渋谷）開業以降、FabCafeは日本国内外で拠点を増やし続けています。2020年時点で世界8カ国・11拠点に広がり、2022年10月現在では世界14拠点に達するグローバルコミュニティとなりました。"},"41":{"comment":"FabCafeはデジタルファブリケーションを活動の出発点としつつ、各拠点それぞれが多様なテクノロジーや文化を巻き込み、その土地ならではのクリエイターコミュニティを形成しています。"},"42":{"comment":"グローバル展開の目的は、世界中のクリエイター同士をネットワークし、新たな価値創造の機会を生み出すことにあります。各地のFabCafeが連携することで、ローカルなイノベーションをグローバルに波及させるプラットフォームとなっています。"},"43":{"comment":"FabCafeの各拠点は、提供するサービスのバランスが異なります。FabCafeの事業は大きく「飲食（ドリンク＆フード）」「デジタル工作機械サービス」「イベント＆ワークショップ」「Fabアイテム販売」の4領域から成りますが​、拠点ごとにどの領域に力を入れるか比重が変わります​。"},"44":{"comment":"例えば、台湾のFabCafe Taipeiではバリスタチャンピオンによるスペシャルティコーヒーが人気で、カフェ（飲食）分野が強みとなっています​。一方、東京（FabCafe Tokyo）では売上構成比がおよそ飲食30%、デジタル工作機械15%、イベント＆ワークショップ55%とイベント系の比重が非常に高くなっています​。"},"45":{"comment":"FabCafeの世界展開第1号となったのは台北（FabCafe Taipei）で、渋谷に次ぐ2店舗目として2013年にオープンしました。東京・台北に続いて、2014年3月27日にはヨーロッパ初となるFabCafe Barcelona（スペイン・バルセロナ）がクリエイター向けコワーキング施設MOB内に正式オープンしています​。"},"46":{"comment":"バルセロナ店は300人以上のメンバーを抱えるコワーキングスペースと併設されており、地域のクリエイターコミュニティとの強固な結びつきを特徴としています。MOB内で生まれるプロジェクトやアイデアとFabCafeの設備・ネットワークが融合することで、新たなコラボレーションが生まれています。"},"47":{"comment":"2014年にはスペインでFabCafe Barcelonaに加え、同国シッチェスにもFabCafeがオープンし、さらに翌2015年にはタイ・バンコクにも開設されるなど、グローバル展開が加速しました。"},"48":{"comment":"2016年時点でもFabCafeネットワークは拡大を続け、フランスのストラスブールやメキシコのモンテレイで新拠点の準備が進められていました。このようにして毎年のように新たな都市へFabCafeが誕生しています。"},"49":{"comment":"メキシコでは、首都メキシコシティにあるアート×サイエンスの実験的ラボ「Membrana Lab」の中にFabCafe Mexico Cityが2022年2月に誕生しました。バイオやXR（Extended Reality）の実験も行う先進的な施設内に設けられ、異分野のクリエイターをつなぐハブとなっています。"},"50":{"comment":"FabCafe Mexico Cityは、アートとテクノロジー、サイエンスを横断するプロジェクトを推進する拠点で、創業メンバー自身がバイオメディアアーティストやキュレーターといった経歴を持ち、アートのエコシステムの持続可能性を探求する活動を展開しています。"},"51":{"comment":"日本国内にもFabCafeは次々と展開しました。2016年春には岐阜県飛騨市古川町に、築100年以上の酒蔵兼木工所だった古民家をリノベーションしたFabCafe Hidaがオープンしています。"},"52":{"comment":"FabCafe Hidaは、飛騨市と民間（株式会社飛騨の森でクマは踊る＝ヒダクマ）およびロフトワークの官民共同事業として設立されました。豊かな森林資源と伝統木工技術を持つ飛騨ならではの文脈で、地域産業の再生とものづくりの革新を目指す拠点です。"},"53":{"comment":"飛騨古川のFabCafe Hidaは、宿泊機能を備えたクリエイター向け滞在施設としての側面も持ちます。クリエイターやデザイナー、建築家が滞在しながら広葉樹林の活用や伝統的な組木技術について学び、新技術との融合を探る「創造的な実験の場」として機能しています。"},"54":{"comment":"さらにFabCafe Hidaでは、企業研修や合宿プログラムの提供も行っており、森を起点にしたものづくりのプロセスを企業の人材育成やプロジェクト創出に役立てるユニークな取り組みを展開しています。"},"55":{"comment":"2017年6月9日には京都・五条エリアにFabCafe Kyotoがオープンしました​。築約120年の町家をリノベーションし、クリエイティブラウンジ「MTRL KYOTO」の1階に併設された形で、東京・飛騨に続く国内3店舗目となりました​。"},"56":{"comment":"FabCafe Kyotoは鴨川近くの文化的なエリアに位置し、「ヒト・モノ・コトが混じりあい化学反応を起こすボーダレスでオープンな場」をコンセプトに掲げています​。和の伝統とデジタルものづくりが交差する拠点として、地元クリエイターや学生、観光客まで幅広い層が集う場となっています。"},"57":{"comment":"2020年秋には名古屋・久屋大通公園内にFabCafe Nagoyaが開設されました。OKB総研（大垣共立銀行グループ）とロフトワークの協働によるプロジェクトで、地元企業と共に新たな価値を創造し地域の未来づくりに貢献する発信・交流拠点として位置づけられています。"},"58":{"comment":"FabCafe Nagoyaは、東海地域有数のものづくり産業や多様な素材（繊維・土・木・食品など）の集積を背景に、それらに関わる人々をつなげることで新たな化学反応を起こすことを目指しています。また、世界中のFabCafeネットワークとロフトワークのグローバルネットワークを活用し、地域内に留まらない国内外との共創も促進しています。"},"59":{"comment":"2022年11月6日、山梨県富士吉田市に国内5拠点目（世界14拠点目）となるFabCafe Fujiがオープンしました。富士山麓の織物産地・富士吉田の伝統産業とクリエイターの共創を目指す拠点であり、地域資源とクリエイティブを掛け合わせた新しい挑戦となっています。"},"60":{"comment":"FabCafe Fujiの店内にはカフェのほか、アート・建築・テキスタイルに関する図書コーナーやリソグラフ印刷を楽しめるスペース、コワーキングスペースが併設されています。富士吉田という1000年以上の織物の歴史を持つ町に根ざし、「テキスタイル」を中心テーマにクリエイティブコミュニティを育むことを目指しています。"},"61":{"comment":"富士吉田では既にクリエイターやアーティストと地元機（はた）屋が協業して新たな織物価値を創造する動きがあり、FabCafe Fujiもそうした伝統と革新の交差点として機能しようとしています。地元住民や織物業界関係者、自治体とも連携しながら、展示会やイベントを通じて地域に新風を吹き込んでいます。"},"62":{"comment":"2025年春には大阪・南森町にFabCafe Osakaがオープン予定です。東京・京都・飛騨・名古屋・富士吉田に続く国内6拠点目で、水の都・大阪らしく蒸留器を使って新たなカルチャーを育む場になる構想が発表されています。"},"63":{"comment":"FabCafe Osakaでは、蒸留技術を活用した実験的プロジェクト（例えば地元の名産や素材を使ったクラフトスピリッツづくり等）が計画されており、都市とローカル文化が交錯する大阪ならではのFabCafeが期待されています。"},"64":{"comment":"Loftwork（ロフトワーク）はFabCafeの運営母体であり、FabCafeを通じて自身のミッションである「オープンコラボレーションによる価値創造」を体現しています。"},"65":{"comment":"株式会社ロフトワークは、ウェブやコンテンツ、コミュニケーション、空間などをオープンコラボレーションでデザインするクリエイティブ・カンパニーであり、FabCafeのほか素材共創プラットフォームのMTRLや共創プログラムのAWRDなどを運営しています。"},"66":{"comment":"ロフトワークの企業ミッションは創業当初から「クリエイティブの流通」を実現することでした​。15年以上にわたりクリエイター向けポータルサイトを運営し、多様なクリエイティブ案件を手掛けてきたロフトワークにとって、FabCafeはリアルな場でクリエイターコミュニティを運営する新たな挑戦でした。"},"67":{"comment":"FabCafeの運営を通じてロフトワークは、自社が培ってきた世界有数のクリエイティブコミュニティとプロジェクトマネジメント力を活かし、人と企業と社会に継続的に向き合い価値を生み続けるビジネスエコシステムの構築を目指しています。"},"68":{"comment":"ロフトワークがFabCafeを運営する理由の一つは、オープンイノベーションの場を自ら持つことで、クライアントワークに留まらないダイナミックな価値創出を可能にするためです。自社のプラットフォームとしてFabCafeを位置づけることで、新規事業創出やコミュニティデザインの実験を行いやすくしています。"},"69":{"comment":"FabCafeはロフトワークにとって、クリエイターと企業・社会をつなぐ共創プラットフォームとして機能します。これによりロフトワークは多様なステークホルダーを巻き込んだプロジェクトを推進しやすくなり、クリエイティブ企業としての提供価値を拡張しています。"},"70":{"comment":"実際、ロフトワーク/FabCafeはこれまでに大企業から自治体、スタートアップ、大学、研究機関、国際機関まで様々なパートナーと協業してプロジェクトを生み出してきました。例えば、UNDP（国連開発計画）や在日イスラエル大使館、慶應義塾大学、沖縄科学技術大学院大学（OIST）、講談社、TSUTAYA（CCC）などとの共創実績があります。"},"71":{"comment":"ロフトワークの社員やFabCafeのチームメンバーは多彩なバックグラウンドを持っており、それぞれがテーマを掲げてコミュニティを形成しています。FabCafeというプラットフォームの上でメンバーが主体的に活動テーマを持ち、プロジェクトやイベントを通じてアウトプットするカルチャーが根付いています。"},"72":{"comment":"FabCafeを運営することでロフトワーク自身も新たな知見を得ています。飲食店運営のノウハウやコミュニティマネジメントの経験は、他のクライアントワークや自社プロジェクトにも活かされ、組織としての進化につながっています。"},"73":{"comment":"ロフトワークにとってFabCafeは、単に事業の一つではなく、自社の理念を実証しアップデートし続ける「実験の場」です。その成功と失敗の両方から得られた学びが、ロフトワーク全体のクリエイティブサービス提供力を高めています。"},"74":{"comment":"FabCafeはロフトワークに属しつつも、FabCafe LLP（有限責任事業組合）という形でグローバル展開の各拠点パートナーと連携する組織体制を取っています。これにより各FabCafeはロフトワークのネットワークとサポートを受けながら自主性も持って運営されています。"},"75":{"comment":"ロフトワーク台湾の創業者でFabCafe TaipeiオーナーでもあるTim Wong氏は「領域を超え、複雑な課題へのソリューションを発見するという私たちのミッションにおいて、FabCafeは重要な役割を果たしている」と述べています。このように各地域のパートナーにとってもFabCafeはミッション達成の手段となっています。"},"76":{"comment":"FabCafeは、その空間とコミュニティ自体がロフトワークにとっての重要なアセット（資産）です。多様な人材ネットワークや知見が集積する場であり、ここから生まれるプロジェクトやプロダクトが数多くあります。"},"77":{"comment":"開業から数年で、FabCafeは延べ17万人以上の来訪者を迎え、そこで生まれた作品（プロジェクト）も1万点を超えました。また開催されたイベントは400件以上に上り、コミュニティ発の活動が活発に行われています。"},"78":{"comment":"こうした実績が評価され、2014年にはグッドデザイン賞（地域・コミュニティづくり）を受賞し、FabCafeはデザインとコミュニティ両面で社会から認知される存在となりました。"},"79":{"comment":"FabCafeでは常に何らかのイベントやワークショップが行われており、そのテーマはテクノロジー、アート、デザイン、サイエンス、ビジネスなど多岐にわたります。これによりコミュニティメンバーは新しい知識やスキルを得る機会に恵まれ、交流を通じて新プロジェクトの種が蒔かれています。"},"80":{"comment":"2014年から始まった「FAB RACERS CUP」は、FabCafe発の代表的な共創プロジェクトです。ミニ四駆やドローンを題材に、大人が真剣に遊びながらエンジニアリングやデザインを学ぶ競技大会・ワークショップシリーズで、年に1回の大会に向けて数ヶ月にわたりハッカソンや勉強会が開催されます。"},"81":{"comment":"FAB RACERSは、未就学児から50代まで幅広い参加者層を持ち、現在300人を超えるコミュニティへと成長しました。自動車・バイクデザイン界を代表するプロも参加しており、トヨタのコンセプトカーを手掛けた根津孝太氏や、本田のヒットバイクをデザインしたやまざきたかゆき氏が大会委員長・副委員長を務めています。"},"82":{"comment":"FAB RACERSには2つのミッションがあります。一つは「遊びを通じて学びを提供すること」で、単なるレース大会ではなくIoTやAIを含む最新技術を学べるハッカソンやワークショップを組み合わせています。もう一つは「企業とクリエイターを結びつけ新たなビジネス機会を探ること」で、仕事とは別の動機で集まったプロたちと企業が楽しく交流しながら新しい可能性を模索しています。"},"83":{"comment":"FAB RACERSのように、FabCafe発のコミュニティが企業との共創プラットフォームに発展した例は他にもあります。例えば、FabCafe Nagoyaでは地元企業と連携した「名古屋共創会議」を開催し、中小企業のデザイン経営に関する具体事例共有や実践的議論の場を提供しています。"},"84":{"comment":"FabCafe Nagoyaの共創会議では、参加企業の担当者同士がコミュニティとしてつながり合い、従来にはない発想や手法で自社の未来像を描くことにチャレンジしています。FabCafeが仲介することで、異業種・異分野の出会いから新規事業のヒントが得られる場となっています。"},"85":{"comment":"FabCafeは企業の新規事業開発やR\u0026Dにも活用されています。2023年にはFabCafe Tokyoにて、新規事業担当者向けに「共創現場のフィールドスタディ」を行うトークイベントが開催され、FabCafeの共創現場を体験的に学ぶ機会を提供しました。"},"86":{"comment":"ロフトワークの共創の知見を共有するこのフィールドスタディでは、FabCafeを舞台に実際のコミュニティドリブンなプロジェクト事例を紹介し、参加者が自社で共創を実践するためのヒントを持ち帰っています。"},"87":{"comment":"FabCafeはまた、クリエイターと社会をつなぐグローバルなプラットフォームとして2012年から「YouFab Global Creative Awards」を主催しています。世界中からデジタルものづくりやメディアアートの作品応募を募り、優れた作品を表彰するアワードで、クリエイター同士の交流や社会との接点を生み出しています。"},"88":{"comment":"YouFab受賞作品には、社会課題にクリエイティブに取り組むものも多く、FabCafeはアワードを通じてグローバルにクリエイターコミュニティを可視化するとともに、企業や自治体が注目すべき先端事例を提示する役割も果たしています。"},"89":{"comment":"FabCafe各拠点は、それぞれ地域の文化や産業資源を活かしたユニークなプロジェクトを展開しています。例えばFabCafe Hidaでは、未利用の国産材を100%使った「森のクレヨン」という木製クレヨンを開発し、日本の森林問題にクリエイティブにアプローチするプロダクトを生み出しました。"},"90":{"comment":"FabCafe Hida発の「森のクレヨン」は、家具や建材に使われない端材や間伐材をクレヨンにアップサイクルしたもので、林野庁との協働プロジェクトとして森の持続可能性への意識啓発にも貢献しています。"},"91":{"comment":"FabCafe MTRL（素材リサーチ拠点）は、FabCafeから派生した取り組みで、素材メーカーとクリエイターの共創を支援するグローバルプラットフォームです。東京や京都のFabCafeにはMTRLコーナーが併設され、最新素材の展示・実験を通じて新規プロジェクト創出が図られています。"},"92":{"comment":"クリエイターと企業の共創事例としては、海洋テクノロジーベンチャーのEverblue TechnologiesとFabCafeが共同で進めたAIデザイン無人艇「ADAM」の開発プロジェクトがあります。これはFabCafeコミュニティのデザイナーやエンジニアが参画し、海洋課題に挑んだ例で、異分野連携によるイノベーション創出のモデルケースとなりました。"},"93":{"comment":"FabCafeでは、最新テクノロジーとクリエイティブの交差点を探るイベントも盛んです。例えば、バイオテクノロジーとデザインの融合を探求するワークショップや、XR技術を用いた新しい表現を試す実験イベントなどが各地で行われています。"},"94":{"comment":"2024年11月には、ロフトワーク・FabCafe・千葉工業大学の3者による初の産学国際連携プロジェクト「Project Apophis」（小惑星探査プロジェクト）が始動しました。2029年に地球に最接近する小惑星アポフィスに探査機を送る計画で、企業・技術・資本・新しい才能や想像力を結集し宇宙領域のビジネス機会を探る壮大なプロジェクトです。"},"95":{"comment":"Project Apophisでは、宇宙・非宇宙の垣根を超えた共創コンソーシアムを形成し、研究者と様々なプロフェッショナルやクリエイターをつなげるコミュニティづくりも目指されています。FabCafeはこのプロジェクトで、異分野の専門家が集う物理的・創造的ハブとして機能し、従来にないコラボレーションを実現しています。"},"96":{"comment":"FabCafe Bangkokでは、国連機関ユネスコと協働し、東北タイの若い女性たちにエンパワーメントの機会を提供するプログラムを展開しました。People・Planet・Profitのトリプルボトムラインを重視したこのイニシアチブは、FabCafeを拠点に地域課題の解決と女性の社会参画をクリエイティブに支援する試みです。"},"97":{"comment":"このバンコクのプロジェクトは、FabCafeが単なる商業空間にとどまらずソーシャルイノベーションのプラットフォームとしても活用されうることを示しました。地域の社会課題に対し、FabCafeコミュニティの若手クリエイターがデザイン思考で取り組み、具体的な成果や提言を生み出しています。"},"98":{"comment":"FabCafeはその空間自体が人と人、人と技術をつなぐ実験装置のような役割を果たしています。例えば店内の大型レーザーカッターや3Dプリンターを通じて生まれたプロジェクトが、そのままスタートアップの事業アイデアに発展したケースもあります（FabCafe Tokyo発のプロジェクトが起業につながった事例も複数存在）。"},"99":{"comment":"FabCafeでは、毎年バレンタイン時期にレーザーカッターでオリジナルチョコレートを作るイベントなど、一般の人が最先端のデジタル工作を楽しめる企画も人気です。こうした身近で遊び心あるイベントを通じて、“Fab”の概念がクリエイター以外にも広がりました。"},"100":{"comment":"開かれたコミュニティであるFabCafeは、新製品やサービスのユーザーテストの場としても活用されています。スタートアップやメーカーが試作したガジェットをFabCafeの利用者に試してもらいフィードバックを得る、といったライブプロトタイピングの光景も日常的に見られます。"},"101":{"comment":"情報発信においてもFabCafeは重要な拠点です。FabCafeの公式ウェブマガジンや各種SNSでは、各拠点で生まれたプロジェクト事例やイベントレポートが随時紹介され、それを見た企業やクリエイターが新たにコミュニティに参加するという好循環が生まれています。"},"102":{"comment":"FabCafeの成功は、クリエイティブコミュニティをビジネスにつなげるモデルケースとして国内外で注目されています。都市型のイノベーション拠点や地方創生のハブを作ろうとする動きにおいて、FabCafeの事例がしばしば引き合いに出され、そのナレッジが共有されています。"}},"overview":"FabCafeは、クリエイティブなものづくりを促進する多機能なプラットフォームとして、地域の特性を活かしたプロジェクトやコミュニティの形成に注力しています。各拠点は、クリエイター同士のコラボレーションを促進し、環境問題への取り組みや新たなビジネス機会の創出を目指しています。ロフトワークの理念に基づき、オープンな場を提供することで、異なるバックグラウンドを持つ人々の交流を促し、社会に新たな価値を提供する実験の場として機能しています。これにより、FabCafeは単なる飲食店を超え、クリエイティブコミュニティのハブとしての役割を果たしています。","config":{"name":"なぜなぜFabCafe","question":"FabCafeとはどんな場所か？","input":"Why-FabCafe","model":"gpt-4o-mini","extraction":{"workers":3,"limit":103,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    response = llm.invoke(messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []","prompt":"/system\n\nあなたはプロのリサーチアシスタントであり、私の仕事は論拠のきれいなデータセットを準備することです。\n\n背景として、私は株式会社ロフトワークが運営するFabCafeについてリサーチしています。\nFabCafeについて書かれた各文を分類するお手伝いをしていただきたいです。\nこれから与えるのはFabCafeについてリサーチした内容のリストです。\nこれから与える投稿をJSONリストとして返してほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"AI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4o-mini"},"clustering":{"clusters":5,"source_code":"import pandas as pd\nimport numpy as np\nfrom importlib import import_module\nimport MeCab\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 必要なモジュールのインポート\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # NEologdの辞書パスを指定\n    neologd_path = '/usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n    mecab = MeCab.Tagger(f\"-d {neologd_path} -Ochasen\")\n\n    # 品詞フィルタリングを行うトークナイザー\n    def tokenizer_mecab(text):\n        node = mecab.parseToNode(text)\n        words = []\n        while node:\n            # 名詞、動詞、形容詞だけを抽出する\n            if node.feature.startswith('名詞') or node.feature.startswith('動詞') or node.feature.startswith('形容詞'):\n                words.append(node.surface)\n            node = node.next\n        return words\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # 日本語のストップワードリストを作成（必要に応じて拡張可能）\n    japanese_stopwords = [\"これ\", \"それ\", \"あれ\", \"こと\", \"もの\", \"ため\", \"よう\", \"さん\", \"する\", \"なる\", \"ある\", \"いる\"]\n\n    # 日本語トークナイザーとストップワードを使用する\n    vectorizer_model = CountVectorizer(tokenizer=tokenizer_mecab, stop_words=japanese_stopwords)\n\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルのフィッティング\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result"},"intro":"なぜなぜFabCafeについてKJ法的手法で整理しました。","output_dir":"Why-FabCafe","previous":{"name":"なぜなぜFabCafe","question":"FabCafeとはどんな場所か？","input":"Why-FabCafe","model":"gpt-4o-mini","extraction":{"workers":3,"limit":103,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    response = llm.invoke(messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []","prompt":"/system\n\nあなたはプロのリサーチアシスタントであり、私の仕事は論拠のきれいなデータセットを準備することです。\n\n背景として、私は株式会社ロフトワークが運営するFabCafeについてリサーチしています。\nFabCafeについて書かれた各文を分類するお手伝いをしていただきたいです。\nこれから与えるのはFabCafeについてリサーチした内容のリストです。\nこれから与える投稿をJSONリストとして返してほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"AI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4o-mini"},"clustering":{"clusters":8,"source_code":"import pandas as pd\nimport numpy as np\nfrom importlib import import_module\nimport MeCab\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 必要なモジュールのインポート\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # NEologdの辞書パスを指定\n    neologd_path = '/usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n    mecab = MeCab.Tagger(f\"-d {neologd_path} -Ochasen\")\n\n    # 品詞フィルタリングを行うトークナイザー\n    def tokenizer_mecab(text):\n        node = mecab.parseToNode(text)\n        words = []\n        while node:\n            # 名詞、動詞、形容詞だけを抽出する\n            if node.feature.startswith('名詞') or node.feature.startswith('動詞') or node.feature.startswith('形容詞'):\n                words.append(node.surface)\n            node = node.next\n        return words\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # 日本語のストップワードリストを作成（必要に応じて拡張可能）\n    japanese_stopwords = [\"これ\", \"それ\", \"あれ\", \"こと\", \"もの\", \"ため\", \"よう\", \"さん\", \"する\", \"なる\", \"ある\", \"いる\"]\n\n    # 日本語トークナイザーとストップワードを使用する\n    vectorizer_model = CountVectorizer(tokenizer=tokenizer_mecab, stop_words=japanese_stopwords)\n\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルのフィッティング\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result"},"intro":"なぜなぜFabCafeについてKJ法的手法で整理しました。","output_dir":"Why-FabCafe","embedding":{"source_code":"from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.chat_models import ChatOpenAI\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeddings_model = OpenAIEmbeddings()\n        embeds = embeddings_model.embed_documents(args)\n\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n\n    return response","prompt":"/system \n\nあなたは、一連のコメントに対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。\nあなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nラベルはクラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\nクラスター外の論点と明確に区別されるようなラベルを回答してください。\n\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？」\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n    return response","prompt":"/system \n\nあなたはシンクタンクで働くリサーチアシスタントのエキスパートです。コメントのリストが渡されます。\nあなたは、主な要点を1~2段落にまとめて回答します。あなたは簡潔で読みやすい短い文章を書くことができます。 \n \n/human\n\n[\n  \"銃による暴力は、私たちの社会における深刻な公衆衛生の危機を構成していると固く信じています。\",\n  \"包括的な銃規制策を通じて、この問題に早急に取り組む必要がある。\",\n  \"すべての銃購入者に対する身元調査の実施を支持します。\",\n  \"アサルト・ウェポンと大容量弾倉の禁止に賛成します。\",\n  \"違法な銃の売買を防ぐため、より厳しい規制を提唱します。\",\n  \"銃の購入プロセスにおいて、精神鑑定を義務付けるべきである。\"\n]\n\n/ai \n\n参加者は、包括的な銃規制を求め、普遍的な身元調査、突撃兵器の禁止、違法な銃売買の抑制、精神衛生評価の優先などを強調した。","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)","prompt":"/system \nあなたはシンクタンクで働く専門家リサーチアシスタントです。あなたのチームは、あるトピックに関する公開コンサルテーションを実施し、さまざまな選択肢のクラスターを分析し始めました。あなたは今、クラスターのリストと各クラスターの簡単な分析を受け取ります。\nあなたの仕事は、その結果を簡潔にまとめることです。あなたの要約は非常に簡潔でなければならず（せいぜい1段落、せいぜい4文）、平凡な表現は避けなければなりません。","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":false,"reason":"nothing changed"},{"step":"embedding","run":false,"reason":"nothing changed"},{"step":"clustering","run":false,"reason":"nothing changed"},{"step":"labelling","run":true,"reason":"some parameters changed: prompt"},{"step":"takeaways","run":false,"reason":"nothing changed"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: labelling, overview"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"completed","start_time":"2025-04-09T16:45:43.832172","completed_jobs":[{"step":"labelling","completed":"2025-04-09T16:45:51.593438","duration":7.756883,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n\n    return response","prompt":"/system \n\nあなたは、一連のコメントに対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。\nあなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nラベルはクラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\nクラスター外の論点と明確に区別されるようなラベルを回答してください。\n\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？」\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-04-09T16:45:56.139667","duration":4.544394,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)","prompt":"/system \nあなたはシンクタンクで働く専門家リサーチアシスタントです。あなたのチームは、あるトピックに関する公開コンサルテーションを実施し、さまざまな選択肢のクラスターを分析し始めました。あなたは今、クラスターのリストと各クラスターの簡単な分析を受け取ります。\nあなたの仕事は、その結果を簡潔にまとめることです。あなたの要約は非常に簡潔でなければならず（せいぜい1段落、せいぜい4文）、平凡な表現は避けなければなりません。","model":"gpt-4o-mini"}},{"step":"aggregation","completed":"2025-04-09T16:45:56.184989","duration":0.043031,"params":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"}},{"step":"visualization","completed":"2025-04-09T16:46:04.735717","duration":8.549836,"params":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"}}],"lock_until":"2025-04-09T16:51:04.736806","current_job":"visualization","current_job_started":"2025-04-09T16:45:56.185917","current_job_progress":null,"current_jop_tasks":null,"previously_completed_jobs":[{"step":"clustering","completed":"2025-04-09T16:41:57.906102","duration":8.901051,"params":{"clusters":8,"source_code":"import pandas as pd\nimport numpy as np\nfrom importlib import import_module\nimport MeCab\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 必要なモジュールのインポート\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # NEologdの辞書パスを指定\n    neologd_path = '/usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n    mecab = MeCab.Tagger(f\"-d {neologd_path} -Ochasen\")\n\n    # 品詞フィルタリングを行うトークナイザー\n    def tokenizer_mecab(text):\n        node = mecab.parseToNode(text)\n        words = []\n        while node:\n            # 名詞、動詞、形容詞だけを抽出する\n            if node.feature.startswith('名詞') or node.feature.startswith('動詞') or node.feature.startswith('形容詞'):\n                words.append(node.surface)\n            node = node.next\n        return words\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # 日本語のストップワードリストを作成（必要に応じて拡張可能）\n    japanese_stopwords = [\"これ\", \"それ\", \"あれ\", \"こと\", \"もの\", \"ため\", \"よう\", \"さん\", \"する\", \"なる\", \"ある\", \"いる\"]\n\n    # 日本語トークナイザーとストップワードを使用する\n    vectorizer_model = CountVectorizer(tokenizer=tokenizer_mecab, stop_words=japanese_stopwords)\n\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルのフィッティング\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result"}},{"step":"takeaways","completed":"2025-04-09T16:42:50.812607","duration":45.881647,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n    return response","prompt":"/system \n\nあなたはシンクタンクで働くリサーチアシスタントのエキスパートです。コメントのリストが渡されます。\nあなたは、主な要点を1~2段落にまとめて回答します。あなたは簡潔で読みやすい短い文章を書くことができます。 \n \n/human\n\n[\n  \"銃による暴力は、私たちの社会における深刻な公衆衛生の危機を構成していると固く信じています。\",\n  \"包括的な銃規制策を通じて、この問題に早急に取り組む必要がある。\",\n  \"すべての銃購入者に対する身元調査の実施を支持します。\",\n  \"アサルト・ウェポンと大容量弾倉の禁止に賛成します。\",\n  \"違法な銃の売買を防ぐため、より厳しい規制を提唱します。\",\n  \"銃の購入プロセスにおいて、精神鑑定を義務付けるべきである。\"\n]\n\n/ai \n\n参加者は、包括的な銃規制を求め、普遍的な身元調査、突撃兵器の禁止、違法な銃売買の抑制、精神衛生評価の優先などを強調した。","model":"gpt-4o-mini"}},{"step":"extraction","completed":"2025-04-09T16:30:32.410751","duration":101.265129,"params":{"workers":3,"limit":103,"source_code":"import os\nimport json\nfrom tqdm import tqdm\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\nimport concurrent.futures\n\n\ndef extraction(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/args.csv\"\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n\n    model = config['extraction']['model']\n    prompt = config['extraction']['prompt']\n    workers = config['extraction']['workers']\n    limit = config['extraction']['limit']\n\n    comment_ids = (comments['comment-id'].values)[:limit]\n    comments.set_index('comment-id', inplace=True)\n    results = pd.DataFrame()\n    update_progress(config, total=len(comment_ids))\n    for i in tqdm(range(0, len(comment_ids), workers)):\n        batch = comment_ids[i: i + workers]\n        batch_inputs = [comments.loc[id]['comment-body'] for id in batch]\n        batch_results = extract_batch(batch_inputs, prompt, model, workers)\n        for comment_id, extracted_args in zip(batch, batch_results):\n            for j, arg in enumerate(extracted_args):\n                new_row = {\"arg-id\": f\"A{comment_id}_{j}\",\n                           \"comment-id\": int(comment_id), \"argument\": arg}\n                results = pd.concat(\n                    [results, pd.DataFrame([new_row])], ignore_index=True)\n        update_progress(config, incr=len(batch))\n    results.to_csv(path, index=False)\n\n\ndef extract_batch(batch, prompt, model, workers):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n        futures = [executor.submit(\n            extract_arguments, input, prompt, model) for input in list(batch)]\n        concurrent.futures.wait(futures)\n        return [future.result() for future in futures]\n\n\ndef extract_arguments(input, prompt, model, retries=3):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    response = llm.invoke(messages(prompt, input)).content.strip()\n    try:\n        obj = json.loads(response)\n        # LLM sometimes returns valid JSON string\n        if isinstance(obj, str):\n            obj = [obj]\n        items = [a.strip() for a in obj]\n        items = filter(None, items)  # omit empty strings\n        return items\n    except json.decoder.JSONDecodeError as e:\n        print(\"JSON error:\", e)\n        print(\"Input was:\", input)\n        print(\"Response was:\", response)\n        if retries \u003e 0:\n            print(\"Retrying...\")\n            return extract_arguments(input, prompt, model, retries - 1)\n        else:\n            print(\"Silently giving up on trying to generate valid list.\")\n            return []","prompt":"/system\n\nあなたはプロのリサーチアシスタントであり、私の仕事は論拠のきれいなデータセットを準備することです。\n\n背景として、私は株式会社ロフトワークが運営するFabCafeについてリサーチしています。\nFabCafeについて書かれた各文を分類するお手伝いをしていただきたいです。\nこれから与えるのはFabCafeについてリサーチした内容のリストです。\nこれから与える投稿をJSONリストとして返してほしい。\n本当に必要な場合は、2つ以上の別々の意見に分けることもできるが、1つのトピックを返すのが最善であることが多いだろう。\n以下にポストを要約する際の事例を挙げます。\nこれらはあくまで文脈の切り離された例であり、この例で与えた文章を返すことはしないでください。\n\n結果は、きちんとフォーマットされた文字列形式（strings）のJSONリストとして返してください。\n\n\n/human\n\n気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\n\n/ai \n\n[\n  \"気候変動を考慮したさらなる風水害対策の強化について、都の具体的な計画をお伺いします。\"\n]\n\n/human \n\n豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\n\n/ai \n\n[\n  \"豪雨対策全般の基本方針の検討を進める中、具体的にどのような施策を作ってほしい。\",\n]\n\n/human\n\nAI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\n\n/ai\n\n[\n  \"AI技術は、そのライフサイクルにおける環境負荷の低減に重点を置いて開発されるべきである。\"\n]\n\n\n/human\n\nいい\n\n/ai\n\n[\n  \"いい\"\n]\n\n/human\n\nあとで読む\n\n/ai\n\n[\n  \"あとで読む\"\n]\n\n/human\n\n読んだ\n\n/ai\n\n[\n  \"読んだ\"\n]\n\n/human\n\n期待\n\n/ai\n\n[\n  \"期待\"\n]","model":"gpt-4o-mini"}},{"step":"embedding","completed":"2025-04-09T16:30:37.594016","duration":5.182026,"params":{"source_code":"from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.chat_models import ChatOpenAI\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeddings_model = OpenAIEmbeddings()\n        embeds = embeddings_model.embed_documents(args)\n\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"}}],"end_time":"2025-04-09T16:46:04.736800"},"embedding":{"source_code":"from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.chat_models import ChatOpenAI\nimport pandas as pd\nfrom tqdm import tqdm\n\n\ndef embedding(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/embeddings.pkl\"\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    embeddings = []\n    for i in tqdm(range(0, len(arguments), 1000)):\n        args = arguments[\"argument\"].tolist()[i: i + 1000]\n        embeddings_model = OpenAIEmbeddings()\n        embeds = embeddings_model.embed_documents(args)\n\n        embeddings.extend(embeds)\n    df = pd.DataFrame(\n        [\n            {\"arg-id\": arguments.iloc[i][\"arg-id\"], \"embedding\": e}\n            for i, e in enumerate(embeddings)\n        ]\n    )\n    df.to_pickle(path)\n"},"labelling":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n\n    return response","prompt":"/system \n\nあなたは、一連のコメントに対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。\nあなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nラベルはクラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\nクラスター外の論点と明確に区別されるようなラベルを回答してください。\n\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？」\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4o-mini"},"takeaways":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n    return response","prompt":"/system \n\nあなたはシンクタンクで働くリサーチアシスタントのエキスパートです。コメントのリストが渡されます。\nあなたは、主な要点を1~2段落にまとめて回答します。あなたは簡潔で読みやすい短い文章を書くことができます。 \n \n/human\n\n[\n  \"銃による暴力は、私たちの社会における深刻な公衆衛生の危機を構成していると固く信じています。\",\n  \"包括的な銃規制策を通じて、この問題に早急に取り組む必要がある。\",\n  \"すべての銃購入者に対する身元調査の実施を支持します。\",\n  \"アサルト・ウェポンと大容量弾倉の禁止に賛成します。\",\n  \"違法な銃の売買を防ぐため、より厳しい規制を提唱します。\",\n  \"銃の購入プロセスにおいて、精神鑑定を義務付けるべきである。\"\n]\n\n/ai \n\n参加者は、包括的な銃規制を求め、普遍的な身元調査、突撃兵器の禁止、違法な銃売買の抑制、精神衛生評価の優先などを強調した。","model":"gpt-4o-mini"},"overview":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)","prompt":"/system \nあなたはシンクタンクで働く専門家リサーチアシスタントです。あなたのチームは、あるトピックに関する公開コンサルテーションを実施し、さまざまな選択肢のクラスターを分析し始めました。あなたは今、クラスターのリストと各クラスターの簡単な分析を受け取ります。\nあなたの仕事は、その結果を簡潔にまとめることです。あなたの要約は非常に簡潔でなければならず（せいぜい1段落、せいぜい4文）、平凡な表現は避けなければなりません。","model":"gpt-4o-mini"},"aggregation":{"source_code":"\"\"\"Generate a convenient JSON output file.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nimport json\n\n\ndef aggregation(config):\n\n    path = f\"outputs/{config['output_dir']}/result.json\"\n\n    results = {\n        \"clusters\": [],\n        \"comments\": {},\n        \"overview\": \"\",\n        \"config\": config,\n    }\n\n    arguments = pd.read_csv(f\"outputs/{config['output_dir']}/args.csv\")\n    arguments.set_index('arg-id', inplace=True)\n\n    comments = pd.read_csv(f\"inputs/{config['input']}.csv\")\n    useful_comment_ids = set(arguments['comment-id'].values)\n    for _, row in comments.iterrows():\n        id = row['comment-id']\n        if id in useful_comment_ids:\n            res = {'comment': row['comment-body']}\n            numeric_cols = ['agrees', 'disagrees']\n            string_cols = ['video', 'interview', 'timestamp']\n            for col in numeric_cols:\n                if col in row:\n                    res[col] = float(row[col])\n            for col in string_cols:\n                if col in row:\n                    res[col] = row[col]\n            results['comments'][str(id)] = res\n\n    languages = list(config.get('translation', {}).get('languages', []))\n    if len(languages) \u003e 0:\n        with open(f\"outputs/{config['output_dir']}/translations.json\") as f:\n            translations = f.read()\n        results['translations'] = json.loads(translations)\n\n    clusters = pd.read_csv(f\"outputs/{config['output_dir']}/clusters.csv\")\n    labels = pd.read_csv(f\"outputs/{config['output_dir']}/labels.csv\")\n    takeaways = pd.read_csv(f\"outputs/{config['output_dir']}/takeaways.csv\")\n    takeaways.set_index('cluster-id', inplace=True)\n\n    with open(f\"outputs/{config['output_dir']}/overview.txt\") as f:\n        overview = f.read()\n    results['overview'] = overview\n\n    for _, row in labels.iterrows():\n        cid = row['cluster-id']\n        label = row['label']\n        arg_rows = clusters[clusters['cluster-id'] == cid]\n        arguments_in_cluster = []\n        for _, arg_row in arg_rows.iterrows():\n            arg_id = arg_row['arg-id']\n            argument = arguments.loc[arg_id]['argument']\n            comment_id = arguments.loc[arg_id]['comment-id']\n            x = float(arg_row['x'])\n            y = float(arg_row['y'])\n            p = float(arg_row['probability'])\n            obj = {\n                'arg_id': arg_id,\n                'argument': argument,\n                'comment_id': str(comment_id),\n                'x': x,\n                'y': y,\n                'p': p,\n            }\n            arguments_in_cluster.append(obj)\n        results['clusters'].append({\n            'cluster': label,\n            'cluster_id': str(cid),\n            'takeaways': takeaways.loc[cid]['takeaways'],\n            'arguments': arguments_in_cluster\n        })\n\n    with open(path, 'w') as file:\n        json.dump(results, file, indent=2)\n"},"visualization":{"replacements":[],"source_code":"\nimport subprocess\n\n\ndef visualization(config):\n    output_dir = config['output_dir']\n    with open(f\"outputs/{output_dir}/result.json\") as f:\n        result = f.read()\n\n    cwd = \"../next-app\"\n    command = f\"REPORT={output_dir} npm run build\"\n\n    try:\n        process = subprocess.Popen(command, shell=True, cwd=cwd, stdout=subprocess.PIPE,\n                                   stderr=subprocess.PIPE, universal_newlines=True)\n        while True:\n            output_line = process.stdout.readline()\n            if output_line == '' and process.poll() is not None:\n                break\n            if output_line:\n                print(output_line.strip())\n        process.wait()\n        errors = process.stderr.read()\n        if errors:\n            print(\"Errors:\")\n            print(errors)\n    except subprocess.CalledProcessError as e:\n        print(\"Error: \", e)\n"},"plan":[{"step":"extraction","run":false,"reason":"nothing changed"},{"step":"embedding","run":false,"reason":"nothing changed"},{"step":"clustering","run":true,"reason":"some parameters changed: clusters"},{"step":"labelling","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"takeaways","run":true,"reason":"some dependent steps will re-run: clustering"},{"step":"overview","run":true,"reason":"some dependent steps will re-run: labelling, takeaways"},{"step":"aggregation","run":true,"reason":"some dependent steps will re-run: clustering, labelling, takeaways, overview"},{"step":"visualization","run":true,"reason":"some dependent steps will re-run: aggregation"}],"status":"running","start_time":"2025-04-09T16:46:49.259918","completed_jobs":[{"step":"clustering","completed":"2025-04-09T16:46:58.986903","duration":9.723684,"params":{"clusters":5,"source_code":"import pandas as pd\nimport numpy as np\nfrom importlib import import_module\nimport MeCab\n\ndef clustering(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/clusters.csv\"\n    arguments_df = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    arguments_array = arguments_df[\"argument\"].values\n\n    embeddings_df = pd.read_pickle(f\"outputs/{dataset}/embeddings.pkl\")\n    embeddings_array = np.asarray(embeddings_df[\"embedding\"].values.tolist())\n    clusters = config['clustering']['clusters']\n\n    result = cluster_embeddings(\n        docs=arguments_array,\n        embeddings=embeddings_array,\n        metadatas={\n            \"arg-id\": arguments_df[\"arg-id\"].values,\n            \"comment-id\": arguments_df[\"comment-id\"].values,\n        },\n        n_topics=clusters,\n    )\n    result.to_csv(path, index=False)\n\n\ndef cluster_embeddings(\n    docs,\n    embeddings,\n    metadatas,\n    min_cluster_size=2,\n    n_components=2,\n    n_topics=6,\n):\n    # 必要なモジュールのインポート\n    SpectralClustering = import_module('sklearn.cluster').SpectralClustering\n    HDBSCAN = import_module('hdbscan').HDBSCAN\n    UMAP = import_module('umap').UMAP\n    CountVectorizer = import_module('sklearn.feature_extraction.text').CountVectorizer\n    BERTopic = import_module('bertopic').BERTopic\n\n    # NEologdの辞書パスを指定\n    neologd_path = '/usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n    mecab = MeCab.Tagger(f\"-d {neologd_path} -Ochasen\")\n\n    # 品詞フィルタリングを行うトークナイザー\n    def tokenizer_mecab(text):\n        node = mecab.parseToNode(text)\n        words = []\n        while node:\n            # 名詞、動詞、形容詞だけを抽出する\n            if node.feature.startswith('名詞') or node.feature.startswith('動詞') or node.feature.startswith('形容詞'):\n                words.append(node.surface)\n            node = node.next\n        return words\n\n    umap_model = UMAP(\n        random_state=42,\n        n_components=n_components,\n    )\n    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size)\n\n    # 日本語のストップワードリストを作成（必要に応じて拡張可能）\n    japanese_stopwords = [\"これ\", \"それ\", \"あれ\", \"こと\", \"もの\", \"ため\", \"よう\", \"さん\", \"する\", \"なる\", \"ある\", \"いる\"]\n\n    # 日本語トークナイザーとストップワードを使用する\n    vectorizer_model = CountVectorizer(tokenizer=tokenizer_mecab, stop_words=japanese_stopwords)\n\n    topic_model = BERTopic(\n        umap_model=umap_model,\n        hdbscan_model=hdbscan_model,\n        vectorizer_model=vectorizer_model,\n        verbose=True,\n    )\n\n    # トピックモデルのフィッティング\n    _, __ = topic_model.fit_transform(docs, embeddings=embeddings)\n\n    n_samples = len(embeddings)\n    n_neighbors = min(n_samples - 1, 10)\n    spectral_model = SpectralClustering(\n        n_clusters=n_topics,\n        affinity=\"nearest_neighbors\",\n        n_neighbors=n_neighbors,\n        random_state=42\n    )\n    umap_embeds = umap_model.fit_transform(embeddings)\n    cluster_labels = spectral_model.fit_predict(umap_embeds)\n\n    result = topic_model.get_document_info(\n        docs=docs,\n        metadata={\n            **metadatas,\n            \"x\": umap_embeds[:, 0],\n            \"y\": umap_embeds[:, 1],\n        },\n    )\n\n    result.columns = [c.lower() for c in result.columns]\n    result = result[['arg-id', 'x', 'y', 'probability']]\n    result['cluster-id'] = cluster_labels\n\n    return result"}},{"step":"labelling","completed":"2025-04-09T16:47:03.737787","duration":4.75016,"params":{"sample_size":30,"source_code":"\"\"\"Create labels for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_openai import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef labelling(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/labels.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['labelling']['sample_size']\n    prompt = config['labelling']['prompt']\n    model = config['labelling']['model']\n\n    question = config['question']\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n\n        args_ids_outside = clusters[clusters['cluster-id']\n                                    != cluster_id]['arg-id'].values\n        args_ids_outside = np.random.choice(args_ids_outside, size=min(\n            len(args_ids_outside), sample_size), replace=False)\n        args_sample_outside = arguments[arguments['arg-id']\n                                        .isin(args_ids_outside)]['argument'].values\n\n        label = generate_label(question, args_sample,\n                               args_sample_outside, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'label': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_label(question, args_sample, args_sample_outside, prompt, model):\n    llm = ChatOpenAI(model=model, temperature=0.0)\n    outside = '\\n * ' + '\\n * '.join(args_sample_outside)\n    inside = '\\n * ' + '\\n * '.join(args_sample)\n    input = f\"Question of the consultation:{question}\\n\\n\" + \\\n        f\"Examples of arguments OUTSIDE the cluster:\\n {outside}\" + \\\n        f\"Examples of arguments INSIDE the cluster:\\n {inside}\"\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n\n    return response","prompt":"/system \n\nあなたは、一連のコメントに対するカテゴリラベルを生成するカテゴリラベリングアシスタントです。\nあなたには、相談の主な質問、クラスタ内の議論のリスト、およびこのクラスタ外の議論のリストが与えられます。あなたはクラスターを要約する1つのカテゴリーラベルで回答します。\n\nラベルはクラスターとその外側にある論点を区別するのに十分な正確さでなければならない。\nクラスター外の論点と明確に区別されるようなラベルを回答してください。\n\n\n/human\n\nコンサルテーションの質問 「英国のEU離脱決定の影響は何だと思いますか？」\n\n関心のあるクラスター以外の論点の例\n\n * エラスムス・プログラムからの除外により、教育・文化交流の機会が制限された。\n * 英国は、国境検問の強化による旅行時間の延長に対処し、通勤客や旅行客に影響を与えた。\n * 環境基準における協力が減少し、気候変動と闘う努力が妨げられた。\n * 相互医療協定の中断により、患者ケアに課題を感じた。\n * Brexit関連の変更により、家族の居住権や市民権の申請が複雑になった。\n * 英国は、共同研究機会の減少により、研究の課題に取り組む世界的な取り組みに支障をきたすことを目の当たりにした。\n * EUの文化助成プログラムからの除外により、創造的なプロジェクトの制限に直面した。\n * 英国は、EUの資金提供の喪失により、慈善活動やコミュニティ支援の後退を目の当たりにした。\n * 消費者保護の弱体化により、国境を越えた紛争解決に課題が生じた。\n * 英国はプロの音楽家としてEU諸国をツアーする際の制限に直面し、キャリアに影響を与えた。\n\nクラスター内部での議論の例\n\n * Brexitによりサプライチェーンが混乱し、企業にとってコスト増と納期遅延につながった。\n * ブレグジットのため、市場の変動や投資・退職金の不確実性に直面した。\n * 新たな関税や通関手続きにより、英国は輸出業者として利益率の低下に対処した。\n * ブレグジット後、企業がEU市場内にとどまるために事業を移転したため、雇用を失った。\n * 英国は輸入品価格の高騰による生活費の増加に苦しんだ。\n * 英国のハイテク産業への投資が減少し、技術革新と雇用機会に影響を与えた。\n * 新たなビザ規制による観光客の減少を目の当たりにし、接客業に影響。\n * ポンド価値の下落により購買力が低下し、旅費が増加した。\n\n\n/ai \n\n財務上のマイナス影響","model":"gpt-4o-mini"}},{"step":"takeaways","completed":"2025-04-09T16:47:29.989115","duration":26.247565,"params":{"sample_size":30,"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef takeaways(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/takeaways.csv\"\n\n    arguments = pd.read_csv(f\"outputs/{dataset}/args.csv\")\n    clusters = pd.read_csv(f\"outputs/{dataset}/clusters.csv\")\n\n    results = pd.DataFrame()\n\n    sample_size = config['takeaways']['sample_size']\n    prompt = config['takeaways']['prompt']\n    model = config['takeaways']['model']\n\n    model = config.get('model_takeaways', config.get('model', 'gpt3.5-turbo'))\n    cluster_ids = clusters['cluster-id'].unique()\n\n    update_progress(config, total=len(cluster_ids))\n\n    for _, cluster_id in tqdm(enumerate(cluster_ids), total=len(cluster_ids)):\n        args_ids = clusters[clusters['cluster-id']\n                            == cluster_id]['arg-id'].values\n        args_ids = np.random.choice(args_ids, size=min(\n            len(args_ids), sample_size), replace=False)\n        args_sample = arguments[arguments['arg-id']\n                                .isin(args_ids)]['argument'].values\n        label = generate_takeaways(args_sample, prompt, model)\n        results = pd.concat([results, pd.DataFrame(\n            [{'cluster-id': cluster_id, 'takeaways': label}])], ignore_index=True)\n        update_progress(config, incr=1)\n\n    results.to_csv(path, index=False)\n\n\ndef generate_takeaways(args_sample, prompt, model):\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    input = \"\\n\".join(args_sample)\n    response = llm.invoke(input=messages(prompt, input)).content.strip()\n    return response","prompt":"/system \n\nあなたはシンクタンクで働くリサーチアシスタントのエキスパートです。コメントのリストが渡されます。\nあなたは、主な要点を1~2段落にまとめて回答します。あなたは簡潔で読みやすい短い文章を書くことができます。 \n \n/human\n\n[\n  \"銃による暴力は、私たちの社会における深刻な公衆衛生の危機を構成していると固く信じています。\",\n  \"包括的な銃規制策を通じて、この問題に早急に取り組む必要がある。\",\n  \"すべての銃購入者に対する身元調査の実施を支持します。\",\n  \"アサルト・ウェポンと大容量弾倉の禁止に賛成します。\",\n  \"違法な銃の売買を防ぐため、より厳しい規制を提唱します。\",\n  \"銃の購入プロセスにおいて、精神鑑定を義務付けるべきである。\"\n]\n\n/ai \n\n参加者は、包括的な銃規制を求め、普遍的な身元調査、突撃兵器の禁止、違法な銃売買の抑制、精神衛生評価の優先などを強調した。","model":"gpt-4o-mini"}},{"step":"overview","completed":"2025-04-09T16:47:33.290536","duration":3.298446,"params":{"source_code":"\"\"\"Create summaries for the clusters.\"\"\"\n\nfrom tqdm import tqdm\nimport os\nfrom typing import List\nimport numpy as np\nimport pandas as pd\nfrom langchain_community.chat_models import ChatOpenAI\nfrom utils import messages, update_progress\n\n\ndef overview(config):\n    dataset = config['output_dir']\n    path = f\"outputs/{dataset}/overview.txt\"\n\n    takeaways = pd.read_csv(f\"outputs/{dataset}/takeaways.csv\")\n    labels = pd.read_csv(f\"outputs/{dataset}/labels.csv\")\n\n    prompt = config['overview']['prompt']\n    model = config['overview']['model']\n\n    ids = labels['cluster-id'].to_list()\n    takeaways.set_index('cluster-id', inplace=True)\n    labels.set_index('cluster-id', inplace=True)\n\n    input = ''\n    for i, id in enumerate(ids):\n        input += f\"# Cluster {i}/{len(ids)}: {labels.loc[id]['label']}\\n\\n\"\n        input += takeaways.loc[id]['takeaways'] + '\\n\\n'\n\n    llm = ChatOpenAI(model_name=model, temperature=0.0)\n    response = llm(messages=messages(prompt, input)).content.strip()\n\n    with open(path, 'w') as file:\n        file.write(response)","prompt":"/system \nあなたはシンクタンクで働く専門家リサーチアシスタントです。あなたのチームは、あるトピックに関する公開コンサルテーションを実施し、さまざまな選択肢のクラスターを分析し始めました。あなたは今、クラスターのリストと各クラスターの簡単な分析を受け取ります。\nあなたの仕事は、その結果を簡潔にまとめることです。あなたの要約は非常に簡潔でなければならず（せいぜい1段落、せいぜい4文）、平凡な表現は避けなければなりません。","model":"gpt-4o-mini"}}],"lock_until":"2025-04-09T16:52:33.294495","current_job":"aggregation","current_job_started":"2025-04-09T16:47:33.294475","current_job_progress":null,"current_jop_tasks":null}}},"__N_SSG":true},"page":"/","query":{},"buildId":"pksff7eA2sBIC06Rubinb","assetPrefix":".","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>